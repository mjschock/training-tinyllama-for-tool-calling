{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2022787",
   "metadata": {
    "papermill": {
     "duration": 0.063638,
     "end_time": "2024-11-21T04:39:33.504391",
     "exception": false,
     "start_time": "2024-11-21T04:39:33.440753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training TinyLlama for Tool-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfc08b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:33.575096Z",
     "iopub.status.busy": "2024-11-21T04:39:33.574793Z",
     "iopub.status.idle": "2024-11-21T04:39:33.587615Z",
     "shell.execute_reply": "2024-11-21T04:39:33.586082Z"
    },
    "papermill": {
     "duration": 0.051904,
     "end_time": "2024-11-21T04:39:33.590154",
     "exception": false,
     "start_time": "2024-11-21T04:39:33.538250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: ~/Projects/training-tinyllama-for-tool-calling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_root = os.getcwd()\n",
    "\n",
    "while not os.path.exists(os.path.join(project_root, \"register_prefect_flow.py\")):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root.replace(os.environ['HOME'], '~')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d818ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:33.659909Z",
     "iopub.status.busy": "2024-11-21T04:39:33.659621Z",
     "iopub.status.idle": "2024-11-21T04:39:40.272977Z",
     "shell.execute_reply": "2024-11-21T04:39:40.271650Z"
    },
    "papermill": {
     "duration": 6.651269,
     "end_time": "2024-11-21T04:39:40.274947",
     "exception": false,
     "start_time": "2024-11-21T04:39:33.623678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r https://raw.githubusercontent.com/mjschock/training-tinyllama-for-tool-calling/refs/heads/main/requirements.txt\n",
    "%pip install -qr $project_root/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce6c208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:40.356231Z",
     "iopub.status.busy": "2024-11-21T04:39:40.355874Z",
     "iopub.status.idle": "2024-11-21T04:39:48.916063Z",
     "shell.execute_reply": "2024-11-21T04:39:48.915286Z"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "5eff0d61-05b4-471c-eea2-c2e84a915109",
    "papermill": {
     "duration": 8.60378,
     "end_time": "2024-11-21T04:39:48.917426",
     "exception": false,
     "start_time": "2024-11-21T04:39:40.313646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import mlflow\n",
    "from mlflow.types.llm import (\n",
    "    ChatChoice,\n",
    "    ChatMessage,\n",
    "    ChatResponse,\n",
    "    FunctionToolCallArguments,\n",
    "    ToolCall,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcef9ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:49.055598Z",
     "iopub.status.busy": "2024-11-21T04:39:49.054460Z",
     "iopub.status.idle": "2024-11-21T04:39:49.137747Z",
     "shell.execute_reply": "2024-11-21T04:39:49.136637Z"
    },
    "papermill": {
     "duration": 0.190525,
     "end_time": "2024-11-21T04:39:49.139193",
     "exception": false,
     "start_time": "2024-11-21T04:39:48.948668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_run started: 476829bc4c43418f8f7b232d05179b4a\n",
      "mlflow_run.to_dictionary():\n",
      "{'data': {'metrics': {},\n",
      "          'params': {},\n",
      "          'tags': {'mlflow.runName': 'useful-cub-600',\n",
      "                   'mlflow.source.name': '/home/mjschock/Projects/training-tinyllama-for-tool-calling/.venv/lib/python3.10/site-packages/ipykernel_launcher.py',\n",
      "                   'mlflow.source.type': 'LOCAL',\n",
      "                   'mlflow.user': 'mjschock'}},\n",
      " 'info': {'artifact_uri': 'file:///home/mjschock/Projects/training-tinyllama-for-tool-calling/mlruns/229981907243359317/476829bc4c43418f8f7b232d05179b4a/artifacts',\n",
      "          'end_time': None,\n",
      "          'experiment_id': '229981907243359317',\n",
      "          'lifecycle_stage': 'active',\n",
      "          'run_id': '476829bc4c43418f8f7b232d05179b4a',\n",
      "          'run_name': 'useful-cub-600',\n",
      "          'run_uuid': '476829bc4c43418f8f7b232d05179b4a',\n",
      "          'start_time': 1732163989125,\n",
      "          'status': 'RUNNING',\n",
      "          'user_id': 'mjschock'}}\n",
      "user_id: mjschock\n"
     ]
    }
   ],
   "source": [
    "mlflow_experiment_name = \"Training TinyLlama for Tool-calling\"\n",
    "mlflow_tracking_uri = f\"file://{project_root}/mlruns\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "assert mlflow.get_tracking_uri() == mlflow_tracking_uri, f\"{mlflow.get_tracking_uri()} != {mlflow_tracking_uri}\"\n",
    "\n",
    "mlflow_experiment = mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow_run = mlflow.active_run()\n",
    "\n",
    "    print(f\"mlflow_run already exists: {mlflow_run.info.run_id}\")\n",
    "\n",
    "else:\n",
    "    mlflow_run = mlflow.start_run(\n",
    "        experiment_id=mlflow_experiment.experiment_id,\n",
    "    )\n",
    "\n",
    "    print(f\"mlflow_run started: {mlflow_run.info.run_id}\")\n",
    "\n",
    "assert mlflow_run.info.experiment_id == mlflow_experiment.experiment_id, f\"{mlflow_run.info.experiment_id} != {mlflow_experiment.experiment_id}\"\n",
    "\n",
    "print('mlflow_run.to_dictionary():')\n",
    "pprint(mlflow_run.to_dictionary())\n",
    "\n",
    "user_id = mlflow_run.info.user_id\n",
    "print('user_id:', user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91e5618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:49.205033Z",
     "iopub.status.busy": "2024-11-21T04:39:49.204721Z",
     "iopub.status.idle": "2024-11-21T04:39:54.546984Z",
     "shell.execute_reply": "2024-11-21T04:39:54.546197Z"
    },
    "papermill": {
     "duration": 5.378722,
     "end_time": "2024-11-21T04:39:54.548357",
     "exception": false,
     "start_time": "2024-11-21T04:39:49.169635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/tinyllama-chat-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "max_seq_length = 4096\n",
    "pretrained_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
    "pretrained_model_namespace = \"TinyLlama\"\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pretrained_model_name_or_path = f\"{pretrained_model_namespace}/{pretrained_model_name}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(project_root, f\"data/06_models/{pretrained_model_namespace}/{pretrained_model_name}\")):\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "    model.save_pretrained(os.path.join(project_root, f\"data/06_models/{pretrained_model_namespace}/{pretrained_model_name}\"))\n",
    "    tokenizer.save_pretrained(os.path.join(project_root, f\"data/06_models/{pretrained_model_namespace}/{pretrained_model_name}\"))\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    max_seq_length=max_seq_length,\n",
    "    # model_name=model_name,\n",
    "    model_name=pretrained_model_name_or_path,\n",
    ")\n",
    "\n",
    "# Save a tokenizer without padding because it is only needed for training\n",
    "# tokenizer_no_pad = AutoTokenizer.from_pretrained(model_name, add_bos_token=True) # https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-peft.html#Save-the-PEFT-Model-to-MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248f8dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:54.613695Z",
     "iopub.status.busy": "2024-11-21T04:39:54.613255Z",
     "iopub.status.idle": "2024-11-21T04:39:54.619148Z",
     "shell.execute_reply": "2024-11-21T04:39:54.618116Z"
    },
    "papermill": {
     "duration": 0.043495,
     "end_time": "2024-11-21T04:39:54.620513",
     "exception": false,
     "start_time": "2024-11-21T04:39:54.577018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd046e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:54.682653Z",
     "iopub.status.busy": "2024-11-21T04:39:54.682365Z",
     "iopub.status.idle": "2024-11-21T04:39:54.691675Z",
     "shell.execute_reply": "2024-11-21T04:39:54.690764Z"
    },
    "papermill": {
     "duration": 0.042004,
     "end_time": "2024-11-21T04:39:54.693262",
     "exception": false,
     "start_time": "2024-11-21T04:39:54.651258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_MESSAGE = {\n",
    "    \"content\": \"You are an AI agent acting as a human assistant.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "tools_template = \"\"\"\n",
    "{\n",
    "  \"tools\": [\n",
    "  {% for tool in tools %}\n",
    "    {\n",
    "      \"function\": {\n",
    "        \"description\": \"{{ tool.function.description }}\",\n",
    "        \"name\": \"{{ tool.function.name }}\",\n",
    "        \"parameters\": {{ tool.function.parameters | tojson }}\n",
    "      },\n",
    "      \"type\": \"{{ tool.type }}\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\n",
    "If you would like to suggest one or more tool calls, please respond in the following format:\n",
    "{\n",
    "  \"finish_reason\": \"tool_calls\",\n",
    "  \"tool_calls\": [\n",
    "  {% for tool in tools %}\n",
    "    {\n",
    "      \"arguments\": \"{\\\\\"parameter_name\\\\\": \\\\\"parameter_value\\\\\"}\",\n",
    "      \"id\": \"call_id\",\n",
    "      \"name\": \"tool_name\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tool_calls_template = \"\"\"\n",
    "{\n",
    "  \"finish_reason\": \"tool_calls\",\n",
    "  \"tool_calls\": [\n",
    "  {% for tool_call in message.tool_calls %}\n",
    "    {\n",
    "      \"arguments\": {{ tool_call.function.arguments | tojson }},\n",
    "      \"id\": \"{{ tool_call.id }}\",\n",
    "      \"name\": \"{{ tool_call.function.name }}\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tool_response_template = \"\"\"\n",
    "{\n",
    "  \"content\": {{ message.content | tojson }},\n",
    "  \"name\": \"{{ message.name }}\",\n",
    "  \"tool_call_id\": \"{{ message.tool_call_id }}\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# if model_name == \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\":\n",
    "if pretrained_model_name == \"TinyLlama-1.1B-Chat-v1.0\":\n",
    "  start_header_id = \"<|\"\n",
    "  end_header_id = \"|>\"\n",
    "\n",
    "else:\n",
    "  start_header_id = \"<|start_header_id|>\"\n",
    "  end_header_id = \"<|end_header_id|>\"\n",
    "\n",
    "role_header_template = start_header_id + \"{{ message.role }}\" + end_header_id + \"{{ '\\n' }}\"\n",
    "assistant_generation_role_header_template = start_header_id + \"assistant\" + end_header_id + \"{{ '\\n' }}\"\n",
    "\n",
    "# Influenced by:\n",
    "# https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models\n",
    "# https://docs.anthropic.com/en/docs/build-with-claude/tool-use\n",
    "# https://github.com/abetlen/llama-cpp-python/blob/7c4aead82d349469bbbe7d8c0f4678825873c039/llama_cpp/llama_chat_format.py#L3387\n",
    "# https://github.com/Mozilla-Ocho/llamafile/blob/66a84d8aea2990895fc4f64786406fea64e79197/llama.cpp/server/server.cpp#L480 (need <|im_start|> b/c Mozilla)\n",
    "# https://github.com/openai/openai-python/blob/120d225b91a8453e15240a49fb1c6794d8119326/chatml.md\n",
    "# https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html#prompt\n",
    "# https://huggingface.co/blog/unified-tool-use\n",
    "chat_template = (\n",
    "    # Configuration and defaults\n",
    "    \"{%- set config = namespace(has_system_message=false, has_tools=false) -%}\"\n",
    "    \"{%- set system_messages = messages | selectattr('role', 'equalto', 'system') | list -%}\"\n",
    "    \"{%- set config.has_system_message = system_messages | length > 0 -%}\"\n",
    "    \"{%- set config.has_tools = tools is defined and tools | length > 0 -%}\"\n",
    "\n",
    "    # Ensure system message exists\n",
    "    \"{%- if not config.has_system_message -%}\"\n",
    "    f'{{%- set messages = [{{ \"content\": \"{DEFAULT_SYSTEM_MESSAGE[\"content\"]}\", \"role\": \"{DEFAULT_SYSTEM_MESSAGE[\"role\"]}\" }}] + messages -%}}'\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Process messages\n",
    "    \"{%- for message in messages -%}\"\n",
    "    # \"<|{{ message.role }}|>{{ '\\n' }}\" # \"<|start_header_id|>{{ message.role }}<|end_header_id|>{{ '\\n' }}\"\n",
    "    # f\"{start_header_id}{{ message.role }}{end_header_id}{{ '\\n' }}\"\n",
    "    # start_header_id + \"{{ message.role }}\" + end_header_id + \"{{ '\\n' }}\"\n",
    "    # TODO: add bos_token if first message?\n",
    "    \"{% if loop.first %}{{ bos_token }}{% endif %}\"f\"{role_header_template}\"\n",
    "\n",
    "    # System message handling\n",
    "    \"{%- if message.role == 'system' -%}\"\n",
    "    \"{{ message.content }}\"\n",
    "    \"{%- if config.has_tools -%}\"\n",
    "    \"{{ '\\n\\n' }}You are aware of the following tools in your environment:\"\n",
    "    f\"{tools_template}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\" # <|eot_id|>\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # User message handling\n",
    "    \"{%- if message.role == 'user' -%}\"\n",
    "    \"{{ message.content }}{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Assistant message handling\n",
    "    \"{%- if message.role == 'assistant' -%}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{%- if message.tool_calls | default(false) -%}\"\n",
    "    f\"{tool_calls_template}\"\n",
    "    \"{%- else -%}\"\n",
    "    \"{{ message.content }}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Tool message handling\n",
    "    \"{%- if message.role == 'tool' -%}\"\n",
    "    f\"{tool_response_template}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{%- endfor -%}\"\n",
    "\n",
    "    # Generation prompt\n",
    "    \"{%- if add_generation_prompt -%}\"\n",
    "    # \"<|assistant|>{{ '\\n' }}\" # <|start_header_id|>assistant<|end_header_id|>\n",
    "    f\"{assistant_generation_role_header_template}\"\n",
    "    \"{%- endif -%}\"\n",
    ")\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template=(\n",
    "#         chat_template,\n",
    "#         \"eos_token\"\n",
    "#     ),\n",
    "#     map_eos_token=True,\n",
    "# )\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18740d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:54.758833Z",
     "iopub.status.busy": "2024-11-21T04:39:54.758581Z",
     "iopub.status.idle": "2024-11-21T04:39:54.767384Z",
     "shell.execute_reply": "2024-11-21T04:39:54.765955Z"
    },
    "papermill": {
     "duration": 0.047278,
     "end_time": "2024-11-21T04:39:54.769354",
     "exception": false,
     "start_time": "2024-11-21T04:39:54.722076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert model.config.bos_token_id == tokenizer.bos_token_id, f\"{model.config.bos_token_id} != {tokenizer.bos_token_id}\"\n",
    "\n",
    "try:\n",
    "    assert model.config.eos_token_id == tokenizer.eos_token_id, f\"{model.config.eos_token_id} != {tokenizer.eos_token_id}\"\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, f\"{model.config.pad_token_id} != {tokenizer.pad_token_id}\"\n",
    "\n",
    "assert model.generation_config.bos_token_id == tokenizer.bos_token_id, f\"{model.generation_config.bos_token_id} != {tokenizer.bos_token_id}\"\n",
    "\n",
    "try:\n",
    "    assert model.generation_config.eos_token_id == tokenizer.eos_token_id, f\"{model.generation_config.eos_token_id} != {tokenizer.eos_token_id}\"\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "assert model.generation_config.pad_token_id == tokenizer.pad_token_id, f\"{model.generation_config.pad_token_id} != {tokenizer.pad_token_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2279008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:54.831792Z",
     "iopub.status.busy": "2024-11-21T04:39:54.831406Z",
     "iopub.status.idle": "2024-11-21T04:39:54.836115Z",
     "shell.execute_reply": "2024-11-21T04:39:54.835397Z"
    },
    "papermill": {
     "duration": 0.03595,
     "end_time": "2024-11-21T04:39:54.837321",
     "exception": false,
     "start_time": "2024-11-21T04:39:54.801371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2a599c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:39:54.904245Z",
     "iopub.status.busy": "2024-11-21T04:39:54.903870Z",
     "iopub.status.idle": "2024-11-21T04:40:27.016871Z",
     "shell.execute_reply": "2024-11-21T04:40:27.015298Z"
    },
    "papermill": {
     "duration": 32.163297,
     "end_time": "2024-11-21T04:40:27.031770",
     "exception": false,
     "start_time": "2024-11-21T04:39:54.868473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will use up to 0.0 out of 15.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 310.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}_unsloth_merged_16bit\",\n",
    "    save_method=\"merged_16bit\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec45dcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:27.350530Z",
     "iopub.status.busy": "2024-11-21T04:40:27.350061Z",
     "iopub.status.idle": "2024-11-21T04:40:33.324949Z",
     "shell.execute_reply": "2024-11-21T04:40:33.324135Z"
    },
    "papermill": {
     "duration": 6.058592,
     "end_time": "2024-11-21T04:40:33.327363",
     "exception": false,
     "start_time": "2024-11-21T04:40:27.268771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': '[]',\n",
       " 'has_parallel_tool_calls': True,\n",
       " 'messages': '[{\"role\": \"user\", \"content\": \"What\\'s the weather like in San Francisco and New York?\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_0\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\\\"location\\\\\": \\\\\"San Francisco, USA\\\\\", \\\\\"format\\\\\": \\\\\"celsius\\\\\"}\"}}, {\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\\\"location\\\\\": \\\\\"New York, USA\\\\\", \\\\\"format\\\\\": \\\\\"celsius\\\\\"}\"}}]}, {\"role\": \"tool\", \"name\": \"get_current_weather\", \"tool_call_id\": \"call_0\", \"content\": \"21.0\"}, {\"role\": \"tool\", \"name\": \"get_current_weather\", \"tool_call_id\": \"call_1\", \"content\": \"18.5\"}, {\"role\": \"assistant\", \"content\": \"The current temperature in San Francisco is 21\\\\u00b0C (70\\\\u00b0F), while in New York it\\'s 18.5\\\\u00b0C (65\\\\u00b0F).\"}]',\n",
       " 'tools': '[{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}}}]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset = load_dataset(\"mjschock/chat_threads\", split=\"train\")\n",
    "train_dataset = load_dataset(f\"{user_id}/chat_threads\", split=\"train\")\n",
    "# validation_dataset = load_dataset(\"mjschock/chat_threads\", split=\"validation\")\n",
    "validation_dataset = load_dataset(f\"{user_id}/chat_threads\", split=\"validation\")\n",
    "# test_dataset = load_dataset(\"mjschock/chat_threads\", split=\"test\")\n",
    "test_dataset = load_dataset(f\"{user_id}/chat_threads\", split=\"test\")\n",
    "\n",
    "test_example = test_dataset[0]\n",
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1157909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.428016Z",
     "iopub.status.busy": "2024-11-21T04:40:33.427697Z",
     "iopub.status.idle": "2024-11-21T04:40:33.433357Z",
     "shell.execute_reply": "2024-11-21T04:40:33.431314Z"
    },
    "papermill": {
     "duration": 0.047959,
     "end_time": "2024-11-21T04:40:33.434947",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.386988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_example_documents = json.loads(test_example[\"documents\"])\n",
    "test_example_messages = json.loads(test_example[\"messages\"])\n",
    "test_example_tools = json.loads(test_example[\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2b1cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.515044Z",
     "iopub.status.busy": "2024-11-21T04:40:33.514713Z",
     "iopub.status.idle": "2024-11-21T04:40:33.520545Z",
     "shell.execute_reply": "2024-11-21T04:40:33.519606Z"
    },
    "papermill": {
     "duration": 0.053545,
     "end_time": "2024-11-21T04:40:33.522051",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.468506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_example_messages:\n",
      "[{'content': \"What's the weather like in San Francisco and New York?\",\n",
      "  'role': 'user'},\n",
      " {'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': '{\"location\": \"San Francisco, '\n",
      "                                            'USA\", \"format\": \"celsius\"}',\n",
      "                               'name': 'get_current_weather'},\n",
      "                  'id': 'call_0',\n",
      "                  'type': 'function'},\n",
      "                 {'function': {'arguments': '{\"location\": \"New York, USA\", '\n",
      "                                            '\"format\": \"celsius\"}',\n",
      "                               'name': 'get_current_weather'},\n",
      "                  'id': 'call_1',\n",
      "                  'type': 'function'}]},\n",
      " {'content': '21.0',\n",
      "  'name': 'get_current_weather',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_0'},\n",
      " {'content': '18.5',\n",
      "  'name': 'get_current_weather',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_1'},\n",
      " {'content': 'The current temperature in San Francisco is 21Â°C (70Â°F), while '\n",
      "             \"in New York it's 18.5Â°C (65Â°F).\",\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print('test_example_messages:')\n",
    "pprint(test_example_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7f8d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.591986Z",
     "iopub.status.busy": "2024-11-21T04:40:33.591334Z",
     "iopub.status.idle": "2024-11-21T04:40:33.596865Z",
     "shell.execute_reply": "2024-11-21T04:40:33.595803Z"
    },
    "papermill": {
     "duration": 0.043256,
     "end_time": "2024-11-21T04:40:33.598512",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.555256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_example_tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'}},\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "print('test_example_tools:')\n",
    "pprint(test_example_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415f20ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.661063Z",
     "iopub.status.busy": "2024-11-21T04:40:33.660774Z",
     "iopub.status.idle": "2024-11-21T04:40:33.691068Z",
     "shell.execute_reply": "2024-11-21T04:40:33.690335Z"
    },
    "papermill": {
     "duration": 0.062041,
     "end_time": "2024-11-21T04:40:33.692437",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.630396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=True,\n",
    "    conversation=test_example_messages[0:1], # Only the user message, note that the system message will automatically be added\n",
    "    documents=test_example_documents,\n",
    "    tools=test_example_tools,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "prompt_and_response = tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=False,\n",
    "    conversation=test_example_messages[0:2], # Only the user and first assistant message, note that the system message will automatically be added\n",
    "    documents=test_example_documents,\n",
    "    tools=test_example_tools,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "response = prompt_and_response.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daa26b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.762352Z",
     "iopub.status.busy": "2024-11-21T04:40:33.762071Z",
     "iopub.status.idle": "2024-11-21T04:40:33.766883Z",
     "shell.execute_reply": "2024-11-21T04:40:33.765397Z"
    },
    "papermill": {
     "duration": 0.044484,
     "end_time": "2024-11-21T04:40:33.768531",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.724047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s><|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s>\n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b154ebfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.836227Z",
     "iopub.status.busy": "2024-11-21T04:40:33.835928Z",
     "iopub.status.idle": "2024-11-21T04:40:33.839969Z",
     "shell.execute_reply": "2024-11-21T04:40:33.839157Z"
    },
    "papermill": {
     "duration": 0.037706,
     "end_time": "2024-11-21T04:40:33.841680",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.803974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (ground truth):\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\",\n",
      "      \"id\": \"call_0\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    },\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\": \\\"New York, USA\\\", \\\"format\\\": \\\"celsius\\\"}\",\n",
      "      \"id\": \"call_1\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    }\n",
      "  ]\n",
      "}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"response (ground truth):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83be9db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:33.908851Z",
     "iopub.status.busy": "2024-11-21T04:40:33.908566Z",
     "iopub.status.idle": "2024-11-21T04:40:34.057814Z",
     "shell.execute_reply": "2024-11-21T04:40:34.056967Z"
    },
    "papermill": {
     "duration": 0.186566,
     "end_time": "2024-11-21T04:40:34.059568",
     "exception": false,
     "start_time": "2024-11-21T04:40:33.873002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset for training.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to preprocess\n",
    "        tokenizer: Tokenizer to use for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: Preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Extract the messages from the example\n",
    "        conversation = examples[\"messages\"]\n",
    "        documents = examples.get(\"documents\", [])\n",
    "        tools = examples.get(\"tools\", [])\n",
    "\n",
    "        # Apply chat template to generate tokenized input and assistant mask\n",
    "        tokenized_output = tokenizer.apply_chat_template(\n",
    "            add_generation_prompt=False,\n",
    "            conversation=json.loads(conversation),\n",
    "            documents=json.loads(documents),\n",
    "            max_length=max_seq_length,\n",
    "            padding=\"longest\",\n",
    "            return_assistant_tokens_mask=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            tokenize=True,\n",
    "            tools=json.loads(tools),\n",
    "            truncation=True,  # TODO: verify we're not truncating anything in the datasets\n",
    "        )\n",
    "\n",
    "        # Extract the input IDs and assistant tokens mask\n",
    "        input_ids = tokenized_output[\"input_ids\"][0]\n",
    "        assistant_masks = torch.tensor(tokenized_output[\"assistant_masks\"])\n",
    "        attention_mask = tokenized_output[\"attention_mask\"][0]\n",
    "\n",
    "        # Use the assistant mask to create labels\n",
    "        labels = torch.where(assistant_masks == 1, input_ids, torch.tensor(-100))\n",
    "\n",
    "        return {\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=False,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )  # TODO: use batched=True\n",
    "\n",
    "\n",
    "tokenized_train_dataset = load_and_preprocess_data(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "tokenized_validation_dataset = load_and_preprocess_data(\n",
    "    validation_dataset,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = load_and_preprocess_data(\n",
    "    test_dataset,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545fd3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:34.122191Z",
     "iopub.status.busy": "2024-11-21T04:40:34.121836Z",
     "iopub.status.idle": "2024-11-21T04:40:34.128501Z",
     "shell.execute_reply": "2024-11-21T04:40:34.127709Z"
    },
    "papermill": {
     "duration": 0.039224,
     "end_time": "2024-11-21T04:40:34.130155",
     "exception": false,
     "start_time": "2024-11-21T04:40:34.090931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_test_prediction():\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=True,\n",
    "        conversation=test_example_messages[0:1], # Only the user message, note that the system message will automatically be added\n",
    "        documents=test_example_documents,\n",
    "        tools=test_example_tools,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        do_sample=False,\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True,\n",
    "        # temperature=0.0,\n",
    "    )\n",
    "\n",
    "    batch_decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    prompt = batch_decoded_outputs[0][0:len(tokenizer.decode(inputs[0]))]\n",
    "    response = batch_decoded_outputs[0][len(tokenizer.decode(inputs[0])):]\n",
    "\n",
    "    return prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01537ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:34.196232Z",
     "iopub.status.busy": "2024-11-21T04:40:34.195867Z",
     "iopub.status.idle": "2024-11-21T04:40:41.019106Z",
     "shell.execute_reply": "2024-11-21T04:40:41.018039Z"
    },
    "papermill": {
     "duration": 6.860864,
     "end_time": "2024-11-21T04:40:41.020753",
     "exception": false,
     "start_time": "2024-11-21T04:40:34.159889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "prompt, response = generate_test_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e862f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:41.109683Z",
     "iopub.status.busy": "2024-11-21T04:40:41.109404Z",
     "iopub.status.idle": "2024-11-21T04:40:41.115106Z",
     "shell.execute_reply": "2024-11-21T04:40:41.113932Z"
    },
    "papermill": {
     "duration": 0.053,
     "end_time": "2024-11-21T04:40:41.116603",
     "exception": false,
     "start_time": "2024-11-21T04:40:41.063603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dd39c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:41.184758Z",
     "iopub.status.busy": "2024-11-21T04:40:41.183753Z",
     "iopub.status.idle": "2024-11-21T04:40:41.188349Z",
     "shell.execute_reply": "2024-11-21T04:40:41.187540Z"
    },
    "papermill": {
     "duration": 0.039548,
     "end_time": "2024-11-21T04:40:41.189583",
     "exception": false,
     "start_time": "2024-11-21T04:40:41.150035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s> <|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s> \n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c6d845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:41.262985Z",
     "iopub.status.busy": "2024-11-21T04:40:41.262650Z",
     "iopub.status.idle": "2024-11-21T04:40:41.266579Z",
     "shell.execute_reply": "2024-11-21T04:40:41.265961Z"
    },
    "papermill": {
     "duration": 0.047679,
     "end_time": "2024-11-21T04:40:41.269065",
     "exception": false,
     "start_time": "2024-11-21T04:40:41.221386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (predicted, before training):\n",
      "I don't have access to real-time weather data, but according to the information available data, the weather in san francisco and new york are both hot and humid. San francisco has a maximum temperature of 328.8Â°c (992.8f) and minimum of 25.8Â°c (439.2f, while new york has maximum of 39.8Â° (434.2f and minimum 2.8Â° (32f).</s>\n"
     ]
    }
   ],
   "source": [
    "print('response (predicted, before training):')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0903b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:41.344829Z",
     "iopub.status.busy": "2024-11-21T04:40:41.344539Z",
     "iopub.status.idle": "2024-11-21T04:40:44.260876Z",
     "shell.execute_reply": "2024-11-21T04:40:44.260019Z"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "b630cc80-ff95-45a2-cc0d-38666010d73b",
    "papermill": {
     "duration": 2.950004,
     "end_time": "2024-11-21T04:40:44.262819",
     "exception": false,
     "start_time": "2024-11-21T04:40:41.312815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjschock/Projects/training-tinyllama-for-tool-calling/.venv/lib/python3.10/site-packages/unsloth/models/_utils.py:697: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    bias=\"none\",\n",
    "    loftq_config=None,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    model=model,\n",
    "    r=16,\n",
    "    random_state=42,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    use_rslora=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72568622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:44.340600Z",
     "iopub.status.busy": "2024-11-21T04:40:44.340047Z",
     "iopub.status.idle": "2024-11-21T04:40:44.346715Z",
     "shell.execute_reply": "2024-11-21T04:40:44.345925Z"
    },
    "papermill": {
     "duration": 0.051499,
     "end_time": "2024-11-21T04:40:44.349023",
     "exception": false,
     "start_time": "2024-11-21T04:40:44.297524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82c5c384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:44.425068Z",
     "iopub.status.busy": "2024-11-21T04:40:44.424676Z",
     "iopub.status.idle": "2024-11-21T04:40:48.210878Z",
     "shell.execute_reply": "2024-11-21T04:40:48.210094Z"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4b809e6d-271f-446f-dec8-abe0d13259f8",
    "papermill": {
     "duration": 3.830699,
     "end_time": "2024-11-21T04:40:48.212409",
     "exception": false,
     "start_time": "2024-11-21T04:40:44.381710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate.combine([\"accuracy\", \"bleu\", \"meteor\", \"rouge\"])\n",
    "metrics_tracker = {}\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction, compute_result: bool) -> Dict:\n",
    "    assert isinstance(\n",
    "        eval_pred, EvalPrediction\n",
    "    ), f\"Expected EvalPrediction, got {type(eval_pred)}\"\n",
    "\n",
    "    all_labels = eval_pred.label_ids\n",
    "    all_preds = eval_pred.predictions\n",
    "    is_last_step = compute_result\n",
    "\n",
    "    all_labels[all_labels == -100] = tokenizer.pad_token_id\n",
    "    references: List[str] = tokenizer.batch_decode(\n",
    "        all_labels, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        all_preds.shape == all_labels.shape\n",
    "    ), f\"Expected predictions and labels to have the same shape, got {all_preds.shape} and {all_labels.shape}\"\n",
    "\n",
    "    predictions: List[str] = tokenizer.batch_decode(\n",
    "        all_preds, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    assert len(predictions) == len(\n",
    "        references\n",
    "    ), f\"Expected predictions and references to have the same length, got {len(predictions)} and {len(references)}\"\n",
    "\n",
    "    eval_batch_metrics = metrics.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "    computed_metrics = {}\n",
    "\n",
    "    for key, value in eval_batch_metrics.items():\n",
    "        if type(value) in [list, np.ndarray]:\n",
    "            value = np.mean(value)\n",
    "\n",
    "        metrics_tracker[key] = np.mean([metrics_tracker.get(key, 0.0), value])\n",
    "        computed_metrics[key] = metrics_tracker[key]\n",
    "\n",
    "        if is_last_step:\n",
    "            metrics_tracker[key] = 0.0\n",
    "\n",
    "    return computed_metrics\n",
    "\n",
    "def preprocess_logits_for_metrics(\n",
    "    logits: torch.Tensor, labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return pred_ids\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    args=SFTConfig(\n",
    "        # auto_find_batch_size=True,\n",
    "        # batch_eval_metrics=True,\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        # eval_accumulation_steps=16,\n",
    "        # eval_on_start=True,\n",
    "        # eval_steps=1.0,\n",
    "        # eval_strategy=\"epoch\",\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        # gradient_accumulation_steps=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # gradient_checkpointing=\"unsloth\",\n",
    "        # learning_rate=5e-05,\n",
    "        learning_rate=2e-4,\n",
    "        # load_best_model_at_end=True,\n",
    "        logging_steps=1.0,\n",
    "        # logging_strategy=\"steps\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # max_seq_length=max_seq_length,\n",
    "        # max_steps=60,\n",
    "        # max_steps=3,\n",
    "        max_steps=1,\n",
    "        # num_of_sequences=1,\n",
    "        # num_train_epochs=3.0,\n",
    "        # num_train_epochs=1.0,\n",
    "        optim=\"adamw_8bit\",\n",
    "        # output_dir=\"outputs\",\n",
    "        # output_dir=\"data/06_models/model\",\n",
    "        # output_dir=\"data/06_models/model/checkpoints\",\n",
    "        # output_dir=os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0_merged_16bit/checkpoints\"),\n",
    "        # output_dir=os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/checkpoints\"),\n",
    "        output_dir=f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}_tool-calling-sft/checkpoints\",\n",
    "        overwrite_output_dir=True,\n",
    "        # packing=False,\n",
    "        per_device_eval_batch_size=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        # push_to_hub=False,\n",
    "        report_to=\"mlflow\",\n",
    "        # save_steps=1.0,\n",
    "        # save_strategy=\"epoch\",\n",
    "        # save_total_limit=1,\n",
    "        seed=42,\n",
    "        warmup_steps=5,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer),\n",
    "    dataset_num_proc=1,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    model=model,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    ")\n",
    "\n",
    "# dpo_trainer = DPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5aa642a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:48.282162Z",
     "iopub.status.busy": "2024-11-21T04:40:48.281675Z",
     "iopub.status.idle": "2024-11-21T04:40:48.288513Z",
     "shell.execute_reply": "2024-11-21T04:40:48.286967Z"
    },
    "papermill": {
     "duration": 0.046146,
     "end_time": "2024-11-21T04:40:48.290673",
     "exception": false,
     "start_time": "2024-11-21T04:40:48.244527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34449d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:48.356738Z",
     "iopub.status.busy": "2024-11-21T04:40:48.356431Z",
     "iopub.status.idle": "2024-11-21T04:40:48.370747Z",
     "shell.execute_reply": "2024-11-21T04:40:48.369745Z"
    },
    "papermill": {
     "duration": 0.049187,
     "end_time": "2024-11-21T04:40:48.372640",
     "exception": false,
     "start_time": "2024-11-21T04:40:48.323453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <|system|>\\nYou are an AI agent acting as a human assistant.\\n\\nYou are aware of the following tools in your environment:\\n{\\n  \"tools\": [\\n    {\\n      \"function\": {\\n        \"description\": \"Calculates the electrostatic potential energy.\",\\n        \"name\": \"calculate_electrostatic_potential_energy\",\\n        \"parameters\": {\"properties\": {\"charge\": {\"type\": \"number\", \"description\": \"The charge of the object, in coulombs.\"}, \"voltage\": {\"type\": \"number\", \"description\": \"The voltage of the object, in volts.\"}}, \"type\": \"object\", \"required\": [\"charge\", \"voltage\"]}\\n      },\\n      \"type\": \"function\"\\n    }\\n  ]\\n}\\n\\nIf you would like to suggest one or more tool calls, please respond in the following format:\\n{\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"parameter_name\\\\\": \\\\\"parameter_value\\\\\"}\",\\n      \"id\": \"call_id\",\\n      \"name\": \"tool_name\"\\n    }\\n  ]\\n}</s> \\n<|user|>\\nI\\'m working on a physics simulation and I have a micro-particle here charged at 7.8 coulombs. It\\'s placed in an electromagnetic field with a voltage of 15.2 volts. Can you calculate the electrostatic potential energy for this particle in the given field?</s> \\n<|assistant|>\\n{\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"charge\\\\\": 7.8, \\\\\"voltage\\\\\": 15.2}\",\\n      \"id\": \"call_0\",\\n      \"name\": \"calculate_electrostatic_potential_energy\"\\n    }\\n  ]\\n}</s> \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first training example. When we decod the input_ids, we'll see the full chat history.\n",
    "\n",
    "tokenizer.decode(sft_trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4de089c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:48.448333Z",
     "iopub.status.busy": "2024-11-21T04:40:48.448086Z",
     "iopub.status.idle": "2024-11-21T04:40:48.459538Z",
     "shell.execute_reply": "2024-11-21T04:40:48.458753Z"
    },
    "papermill": {
     "duration": 0.053662,
     "end_time": "2024-11-21T04:40:48.460875",
     "exception": false,
     "start_time": "2024-11-21T04:40:48.407213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"charge\\\\\": 7.8, \\\\\"voltage\\\\\": 15.2}\",\\n      \"id\": \"call_0\",\\n      \"name\": \"calculate_electrostatic_potential_energy\"\\n    }\\n  ]\\n}      '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's take a look at the labels. The labels are the same as the input_ids, except that the assistant tokens are replaced with -100. This is because we want to predict the assistant tokens.\n",
    "\n",
    "space = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
    "tokenizer.decode(\n",
    "    [space if x == -100 else x for x in sft_trainer.train_dataset[0][\"labels\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e44228da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:48.528001Z",
     "iopub.status.busy": "2024-11-21T04:40:48.527298Z",
     "iopub.status.idle": "2024-11-21T04:40:48.533590Z",
     "shell.execute_reply": "2024-11-21T04:40:48.532561Z"
    },
    "papermill": {
     "duration": 0.04021,
     "end_time": "2024-11-21T04:40:48.534741",
     "exception": false,
     "start_time": "2024-11-21T04:40:48.494531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce GTX 1050 Ti. Max memory = 3.94 GB.\n",
      "2.592 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e45b8339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:40:48.608002Z",
     "iopub.status.busy": "2024-11-21T04:40:48.607329Z",
     "iopub.status.idle": "2024-11-21T04:42:58.969002Z",
     "shell.execute_reply": "2024-11-21T04:42:58.967616Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "3cf26aac-6042-4458-c4a6-d8849efb6a95",
    "papermill": {
     "duration": 130.404435,
     "end_time": "2024-11-21T04:42:58.971527",
     "exception": false,
     "start_time": "2024-11-21T04:40:48.567092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 240 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 4 | Total steps = 1\n",
      " \"-____-\"     Number of trainable parameters = 78,151,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.264400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_stats:\n",
      "TrainOutput(global_step=1, training_loss=2.264357328414917, metrics={'train_runtime': 127.9528, 'train_samples_per_second': 0.031, 'train_steps_per_second': 0.008, 'total_flos': 38026407051264.0, 'train_loss': 2.264357328414917, 'epoch': 0.016666666666666666})\n"
     ]
    }
   ],
   "source": [
    "# with mlflow.start_run(\n",
    "    # experiment_id=experiment.experiment_id,\n",
    "#     nested=True,\n",
    "# ):\n",
    "trainer_stats = sft_trainer.train(\n",
    "    resume_from_checkpoint=False,\n",
    "    trial=None,\n",
    ")\n",
    "\n",
    "print('trainer_stats:')\n",
    "pprint(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15f8a81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:42:59.077847Z",
     "iopub.status.busy": "2024-11-21T04:42:59.077562Z",
     "iopub.status.idle": "2024-11-21T04:42:59.086548Z",
     "shell.execute_reply": "2024-11-21T04:42:59.085405Z"
    },
    "papermill": {
     "duration": 0.069163,
     "end_time": "2024-11-21T04:42:59.091585",
     "exception": false,
     "start_time": "2024-11-21T04:42:59.022422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.9528 seconds used for training.\n",
      "2.13 minutes used for training.\n",
      "Peak reserved memory = 2.693 GB.\n",
      "Peak reserved memory for training = 0.101 GB.\n",
      "Peak reserved memory % of max memory = 68.35 %.\n",
      "Peak reserved memory for training % of max memory = 2.563 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1c30e20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:42:59.203661Z",
     "iopub.status.busy": "2024-11-21T04:42:59.200891Z",
     "iopub.status.idle": "2024-11-21T04:42:59.209322Z",
     "shell.execute_reply": "2024-11-21T04:42:59.208003Z"
    },
    "papermill": {
     "duration": 0.062772,
     "end_time": "2024-11-21T04:42:59.215883",
     "exception": false,
     "start_time": "2024-11-21T04:42:59.153111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bb346ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:42:59.305734Z",
     "iopub.status.busy": "2024-11-21T04:42:59.305456Z",
     "iopub.status.idle": "2024-11-21T04:43:05.632090Z",
     "shell.execute_reply": "2024-11-21T04:43:05.631088Z"
    },
    "papermill": {
     "duration": 6.370298,
     "end_time": "2024-11-21T04:43:05.633496",
     "exception": false,
     "start_time": "2024-11-21T04:42:59.263198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt, response = generate_test_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "436a1262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:05.702858Z",
     "iopub.status.busy": "2024-11-21T04:43:05.702582Z",
     "iopub.status.idle": "2024-11-21T04:43:05.706883Z",
     "shell.execute_reply": "2024-11-21T04:43:05.706095Z"
    },
    "papermill": {
     "duration": 0.041588,
     "end_time": "2024-11-21T04:43:05.708289",
     "exception": false,
     "start_time": "2024-11-21T04:43:05.666701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s> <|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s> \n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ef9ec8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:05.782790Z",
     "iopub.status.busy": "2024-11-21T04:43:05.782292Z",
     "iopub.status.idle": "2024-11-21T04:43:05.788364Z",
     "shell.execute_reply": "2024-11-21T04:43:05.787452Z"
    },
    "papermill": {
     "duration": 0.047127,
     "end_time": "2024-11-21T04:43:05.789683",
     "exception": false,
     "start_time": "2024-11-21T04:43:05.742556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (predicted, after training):\n",
      "I don't have access to real-time weather data, but according to the information available data, the weather in san francisco and new york are both hot and humid. San francisco has a maximum temperature of 328.8Â°c (992.8f) and minimum of 25.8Â°c (439.2f, while new york has maximum of 39.8Â° (434.2f and minimum 2.8Â° (32f).</s>\n"
     ]
    }
   ],
   "source": [
    "print('response (predicted, after training):')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "675562d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:05.885219Z",
     "iopub.status.busy": "2024-11-21T04:43:05.884285Z",
     "iopub.status.idle": "2024-11-21T04:43:05.888691Z",
     "shell.execute_reply": "2024-11-21T04:43:05.887983Z"
    },
    "papermill": {
     "duration": 0.05611,
     "end_time": "2024-11-21T04:43:05.890073",
     "exception": false,
     "start_time": "2024-11-21T04:43:05.833963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a92a7d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:05.964061Z",
     "iopub.status.busy": "2024-11-21T04:43:05.963499Z",
     "iopub.status.idle": "2024-11-21T04:43:07.477872Z",
     "shell.execute_reply": "2024-11-21T04:43:07.476835Z"
    },
    "papermill": {
     "duration": 1.553973,
     "end_time": "2024-11-21T04:43:07.479826",
     "exception": false,
     "start_time": "2024-11-21T04:43:05.925853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    # model.save_pretrained(\"lora_model\")  # Local saving\n",
    "    # model.save_pretrained(\"data/06_models/model/lora\")  # Local saving\n",
    "    # model.save_pretrained(os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"))  # Local saving\n",
    "    model.save_pretrained(f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/lora\"), # Local saving\n",
    "    # tokenizer.save_pretrained(\"lora_model\")\n",
    "    # tokenizer.save_pretrained(\"data/06_models/model/lora\")\n",
    "    # tokenizer.save_pretrained(os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"))\n",
    "    tokenizer.save_pretrained(f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/lora\") # Local saving\n",
    "\n",
    "else:\n",
    "    # TODO: push to hub\n",
    "    raise NotImplementedError(\"Pushing to hub is not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4edb01e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:07.638967Z",
     "iopub.status.busy": "2024-11-21T04:43:07.638231Z",
     "iopub.status.idle": "2024-11-21T04:43:07.643249Z",
     "shell.execute_reply": "2024-11-21T04:43:07.642568Z"
    },
    "papermill": {
     "duration": 0.067375,
     "end_time": "2024-11-21T04:43:07.644505",
     "exception": false,
     "start_time": "2024-11-21T04:43:07.577130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f56b493f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:07.761711Z",
     "iopub.status.busy": "2024-11-21T04:43:07.761248Z",
     "iopub.status.idle": "2024-11-21T04:43:33.258622Z",
     "shell.execute_reply": "2024-11-21T04:43:33.257762Z"
    },
    "papermill": {
     "duration": 25.55725,
     "end_time": "2024-11-21T04:43:33.261626",
     "exception": false,
     "start_time": "2024-11-21T04:43:07.704376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 0.0 out of 15.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                 | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                            | 2/22 [00:00<00:01, 19.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                         | 5/22 [00:00<00:00, 24.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                       | 8/22 [00:00<00:00, 25.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                    | 11/22 [00:00<00:00, 26.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 14/22 [00:00<00:00, 26.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 17/22 [00:00<00:00, 25.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 20/22 [00:00<00:00, 26.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 25.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model.save_pretrained_merged(\n",
    "        # os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/merged_16bit\"),\n",
    "        f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/unsloth_merged_16bit\",\n",
    "        save_method=\"merged_16bit\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    # model.save_pretrained_merged(\n",
    "    #     os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/merged_4bit\"),\n",
    "    #     save_method=\"merged_4bit\",\n",
    "    #     tokenizer=tokenizer,\n",
    "    # )\n",
    "    model.save_pretrained_merged(\n",
    "        f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/unsloth_lora\",\n",
    "        save_method=\"lora\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Pushing to hub is not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a207b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:33.358198Z",
     "iopub.status.busy": "2024-11-21T04:43:33.357829Z",
     "iopub.status.idle": "2024-11-21T04:43:33.363012Z",
     "shell.execute_reply": "2024-11-21T04:43:33.361292Z"
    },
    "papermill": {
     "duration": 0.049833,
     "end_time": "2024-11-21T04:43:33.365052",
     "exception": false,
     "start_time": "2024-11-21T04:43:33.315219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "314f2a01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:33.448795Z",
     "iopub.status.busy": "2024-11-21T04:43:33.448034Z",
     "iopub.status.idle": "2024-11-21T04:43:33.455738Z",
     "shell.execute_reply": "2024-11-21T04:43:33.454824Z"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "5b71f982-38c0-44c8-a4e5-58cd20b5a585",
    "papermill": {
     "duration": 0.052482,
     "end_time": "2024-11-21T04:43:33.457704",
     "exception": false,
     "start_time": "2024-11-21T04:43:33.405222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def chat_completion_request(\n",
    "#     documents: list,\n",
    "#     messages: list,\n",
    "#     tools: list,\n",
    "# ):\n",
    "#     FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "#     inputs = tokenizer.apply_chat_template(\n",
    "#         add_generation_prompt=True,\n",
    "#         conversation=messages,\n",
    "#         documents=documents,\n",
    "#         return_tensors=\"pt\",\n",
    "#         tokenize=True,\n",
    "#         tools=tools,\n",
    "#     ).to(device)\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         do_sample=False,\n",
    "#         input_ids=inputs,\n",
    "#         max_new_tokens=256,\n",
    "#         use_cache=True,\n",
    "#         # temperature=0.0,\n",
    "#     )\n",
    "\n",
    "#     batch_decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "#     choices: List[ChatChoice] = []\n",
    "\n",
    "#     for i in range(len(batch_decoded_outputs)):\n",
    "#         response = batch_decoded_outputs[i][len(tokenizer.decode(inputs[i])):].replace(tokenizer.eos_token, \"\") # TODO: skip special tokens when decoding instead?\n",
    "\n",
    "#         try:\n",
    "#             response = json.loads(response)\n",
    "\n",
    "#             finish_reason: str = response.get(\"finish_reason\")\n",
    "#             tool_calls_json = response.get(\"tool_calls\")\n",
    "#             tool_calls: List[ToolCall] = []\n",
    "\n",
    "#             for tool_call_json in tool_calls_json:\n",
    "#                 tool_call = ToolCall(\n",
    "#                     function=FunctionToolCallArguments(\n",
    "#                         arguments=tool_call_json.get(\"arguments\"),\n",
    "#                         name=tool_call_json.get(\"name\"),\n",
    "#                     ),\n",
    "#                     id=tool_call_json.get(\"id\"),\n",
    "#                     type=\"function\",\n",
    "#                 )\n",
    "\n",
    "#                 tool_calls.append(tool_call)\n",
    "\n",
    "#             message: ChatMessage = ChatMessage(\n",
    "#                 role=\"assistant\",\n",
    "#                 tool_calls=tool_calls,\n",
    "#             )\n",
    "\n",
    "#         except json.JSONDecodeError:\n",
    "#             finish_reason: str = \"stop\"\n",
    "#             message: ChatMessage = ChatMessage(\n",
    "#                 role=\"assistant\",\n",
    "#                 content=response,\n",
    "#             )\n",
    "\n",
    "#         choices.append(\n",
    "#             ChatChoice(\n",
    "#                 index=i,\n",
    "#                 finish_reason=finish_reason,\n",
    "#                 logprobs=None,\n",
    "#                 message=message,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     return ChatResponse(\n",
    "#         choices=choices,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7b56614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:33.555727Z",
     "iopub.status.busy": "2024-11-21T04:43:33.555114Z",
     "iopub.status.idle": "2024-11-21T04:43:33.560005Z",
     "shell.execute_reply": "2024-11-21T04:43:33.559148Z"
    },
    "papermill": {
     "duration": 0.047775,
     "end_time": "2024-11-21T04:43:33.562620",
     "exception": false,
     "start_time": "2024-11-21T04:43:33.514845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_path = f\"{project_root}/src/training_tinyllama_for_tool_calling/services/agent.py\"\n",
    "\n",
    "os.makedirs(Path(code_path).parent, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "720c4219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:33.640427Z",
     "iopub.status.busy": "2024-11-21T04:43:33.639642Z",
     "iopub.status.idle": "2024-11-21T04:43:33.651834Z",
     "shell.execute_reply": "2024-11-21T04:43:33.650960Z"
    },
    "papermill": {
     "duration": 0.052433,
     "end_time": "2024-11-21T04:43:33.653772",
     "exception": false,
     "start_time": "2024-11-21T04:43:33.601339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/mjschock/Projects/training-tinyllama-for-tool-calling/src/training_tinyllama_for_tool_calling/services/agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $code_path\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "import evaluate\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from mlflow.models import set_model\n",
    "from mlflow.pyfunc import ChatModel\n",
    "from mlflow.types.llm import (\n",
    "    ChatChoice,\n",
    "    ChatMessage,\n",
    "    ChatParams,\n",
    "    ChatResponse,\n",
    "    FunctionToolCallArguments,\n",
    "    ToolCall,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "project_root = os.getcwd()\n",
    "\n",
    "while not os.path.exists(os.path.join(project_root, \"register_prefect_flow.py\")):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "max_seq_length = 4096\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"data/06_models/model/lora\"\n",
    "# model_name = \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0_lora_sft\"\n",
    "# model_name = f\"{project_root}/data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"\n",
    "# model_name = f\"{project_root}/data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"\n",
    "\n",
    "class ModelClient:\n",
    "    def __init__(self):\n",
    "        user_id = \"mjschock\" # TODO: get this dynamically\n",
    "        pretrained_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
    "        model_name = f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/lora\"\n",
    "        # model_name = f\"{project_root}/data/06_models/{user_id}/{pretrained_model_name}-tool-calling-sft/unsloth_lora\" # TODO:Maybe this would work better for using the model rather than code path?\n",
    "\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            max_seq_length=max_seq_length,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def chat_completion_request(\n",
    "        self,\n",
    "        documents: list,\n",
    "        messages: list,\n",
    "        tools: list,\n",
    "    ):\n",
    "        # FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            add_generation_prompt=True,\n",
    "            conversation=messages,\n",
    "            documents=documents,\n",
    "            return_tensors=\"pt\",\n",
    "            tokenize=True,\n",
    "            tools=tools,\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            do_sample=False,\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=256,\n",
    "            use_cache=True,\n",
    "            # temperature=0.0,\n",
    "        )\n",
    "\n",
    "        batch_decoded_outputs = self.tokenizer.batch_decode(outputs)\n",
    "\n",
    "        choices: List[ChatChoice] = []\n",
    "\n",
    "        for i in range(len(batch_decoded_outputs)):\n",
    "            response = batch_decoded_outputs[i][\n",
    "                len(self.tokenizer.decode(inputs[i])) :\n",
    "            ].replace(\n",
    "                self.tokenizer.eos_token, \"\"\n",
    "            )  # TODO: skip special tokens when decoding instead?\n",
    "\n",
    "            try:\n",
    "                response = json.loads(response)\n",
    "\n",
    "                finish_reason: str = response.get(\"finish_reason\")\n",
    "                tool_calls_json = response.get(\"tool_calls\")\n",
    "                tool_calls: List[ToolCall] = []\n",
    "\n",
    "                for tool_call_json in tool_calls_json:\n",
    "                    tool_call = ToolCall(\n",
    "                        function=FunctionToolCallArguments(\n",
    "                            arguments=tool_call_json.get(\"arguments\"),\n",
    "                            name=tool_call_json.get(\"name\"),\n",
    "                        ),\n",
    "                        id=tool_call_json.get(\"id\"),\n",
    "                        type=\"function\",\n",
    "                    )\n",
    "\n",
    "                    tool_calls.append(tool_call)\n",
    "\n",
    "                message: ChatMessage = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    tool_calls=tool_calls,\n",
    "                )\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                finish_reason: str = \"stop\"\n",
    "                message: ChatMessage = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=response,\n",
    "                )\n",
    "\n",
    "            choices.append(\n",
    "                ChatChoice(\n",
    "                    index=i,\n",
    "                    finish_reason=finish_reason,\n",
    "                    logprobs=None,\n",
    "                    message=message,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return ChatResponse(\n",
    "            choices=choices,\n",
    "        )\n",
    "\n",
    "\n",
    "class Agent(ChatModel):\n",
    "    def __init__(self):\n",
    "        # self.model_name = \"llama3.2:1b\"\n",
    "        self.client = None\n",
    "\n",
    "    def load_context(self, context):\n",
    "        # self.model_name = \"llama3.2:1b\"\n",
    "        # self.client = ollama.Client()\n",
    "        print('=== load_context ===')\n",
    "        print('context:', context)\n",
    "\n",
    "        self.client = ModelClient()\n",
    "\n",
    "    # the core method that needs to be implemented. this function\n",
    "    # will be called every time a user sends messages to our model\n",
    "    # @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, context, messages: list[ChatMessage], params: ChatParams):\n",
    "        # instantiate the OpenAI client\n",
    "        # client = OpenAI()\n",
    "\n",
    "        # convert the messages to a format that the OpenAI API expects\n",
    "        messages = [m.to_dict() for m in messages]\n",
    "\n",
    "        print(\"params:\")\n",
    "        pprint(params.to_dict())\n",
    "\n",
    "        tools = params.tools or []\n",
    "\n",
    "        print(\"tools:\")\n",
    "        pprint(tools)\n",
    "\n",
    "        tools = [t.to_dict() for t in tools]\n",
    "\n",
    "        print(\"tools:\")\n",
    "        pprint(tools)\n",
    "\n",
    "        # call the OpenAI API\n",
    "        # response = client.chat.completions.create(\n",
    "        #     model=\"gpt-4o-mini\",\n",
    "        #     messages=messages,\n",
    "        #     # pass the tools in the request\n",
    "        #     tools=self.tools,\n",
    "        # )\n",
    "\n",
    "        response = self.client.chat_completion_request(\n",
    "            documents=[],  # we don't need documents for this example\n",
    "            messages=messages,\n",
    "            # tools=self.tools,\n",
    "            # tools=[],\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        # return the result as a ChatResponse, as this\n",
    "        # is the expected output of the predict method\n",
    "        return ChatResponse.from_dict(response.to_dict())\n",
    "\n",
    "\n",
    "set_model(Agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f296fea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:33.736420Z",
     "iopub.status.busy": "2024-11-21T04:43:33.736169Z",
     "iopub.status.idle": "2024-11-21T04:43:34.005132Z",
     "shell.execute_reply": "2024-11-21T04:43:34.003799Z"
    },
    "papermill": {
     "duration": 0.313557,
     "end_time": "2024-11-21T04:43:34.007391",
     "exception": false,
     "start_time": "2024-11-21T04:43:33.693834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34229"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del sft_trainer\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del tokenizer\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "365b6763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:43:34.095504Z",
     "iopub.status.busy": "2024-11-21T04:43:34.094785Z",
     "iopub.status.idle": "2024-11-21T04:44:38.372663Z",
     "shell.execute_reply": "2024-11-21T04:44:38.371388Z"
    },
    "papermill": {
     "duration": 64.325171,
     "end_time": "2024-11-21T04:44:38.375532",
     "exception": false,
     "start_time": "2024-11-21T04:43:34.050361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/20 20:43:34 INFO mlflow.pyfunc: Predicting on input example to validate output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/mjschock/Projects/training-tinyllama-for-tool-calling\n",
      "=== load_context ===\n",
      "context: <mlflow.pyfunc.model.PythonModelContext object at 0x7ce8e8377040>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/tinyllama-chat-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "{'n': 1,\n",
      " 'stream': False,\n",
      " 'temperature': 1.0,\n",
      " 'tools': [{'function': {'description': 'Get the current weather',\n",
      "                         'name': 'get_current_weather',\n",
      "                         'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                           'fahrenheit'],\n",
      "                                                                  'type': 'string'},\n",
      "                                                       'location': {'description': 'The '\n",
      "                                                                                   'city '\n",
      "                                                                                   'and '\n",
      "                                                                                   'country, '\n",
      "                                                                                   'eg. '\n",
      "                                                                                   'San '\n",
      "                                                                                   'Francisco, '\n",
      "                                                                                   'USA',\n",
      "                                                                    'type': 'string'}},\n",
      "                                        'required': ['location', 'format'],\n",
      "                                        'type': 'object'},\n",
      "                         'strict': False},\n",
      "            'type': 'function'}]}\n",
      "tools:\n",
      "[ToolDefinition(function=FunctionToolDefinition(name='get_current_weather',\n",
      "                                                description='Get the current '\n",
      "                                                            'weather',\n",
      "                                                parameters=ToolParamsSchema(properties={'format': ParamProperty(type='string',\n",
      "                                                                                                                description=None,\n",
      "                                                                                                                enum=['celsius',\n",
      "                                                                                                                      'fahrenheit'],\n",
      "                                                                                                                items=None),\n",
      "                                                                                        'location': ParamProperty(type='string',\n",
      "                                                                                                                  description='The '\n",
      "                                                                                                                              'city '\n",
      "                                                                                                                              'and '\n",
      "                                                                                                                              'country, '\n",
      "                                                                                                                              'eg. '\n",
      "                                                                                                                              'San '\n",
      "                                                                                                                              'Francisco, '\n",
      "                                                                                                                              'USA',\n",
      "                                                                                                                  enum=None,\n",
      "                                                                                                                  items=None)},\n",
      "                                                                            type='object',\n",
      "                                                                            required=['location',\n",
      "                                                                                      'format'],\n",
      "                                                                            additionalProperties=None),\n",
      "                                                strict=False),\n",
      "                type='function')]\n",
      "tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'},\n",
      "               'strict': False},\n",
      "  'type': 'function'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3469ab9a1cf3425b80f0cda27b277732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/mjschock/Projects/training-tinyllama-for-tool-calling\n",
      "=== load_context ===\n",
      "context: <mlflow.pyfunc.model.PythonModelContext object at 0x7ce8d8384520>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "{'n': 1,\n",
      " 'stream': False,\n",
      " 'temperature': 1.0,\n",
      " 'tools': [{'function': {'description': 'Get the current weather',\n",
      "                         'name': 'get_current_weather',\n",
      "                         'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                           'fahrenheit'],\n",
      "                                                                  'type': 'string'},\n",
      "                                                       'location': {'description': 'The '\n",
      "                                                                                   'city '\n",
      "                                                                                   'and '\n",
      "                                                                                   'country, '\n",
      "                                                                                   'eg. '\n",
      "                                                                                   'San '\n",
      "                                                                                   'Francisco, '\n",
      "                                                                                   'USA',\n",
      "                                                                    'type': 'string'}},\n",
      "                                        'required': ['location', 'format'],\n",
      "                                        'type': 'object'},\n",
      "                         'strict': False},\n",
      "            'type': 'function'}]}\n",
      "tools:\n",
      "[ToolDefinition(function=FunctionToolDefinition(name='get_current_weather',\n",
      "                                                description='Get the current '\n",
      "                                                            'weather',\n",
      "                                                parameters=ToolParamsSchema(properties={'format': ParamProperty(type='string',\n",
      "                                                                                                                description=None,\n",
      "                                                                                                                enum=['celsius',\n",
      "                                                                                                                      'fahrenheit'],\n",
      "                                                                                                                items=None),\n",
      "                                                                                        'location': ParamProperty(type='string',\n",
      "                                                                                                                  description='The '\n",
      "                                                                                                                              'city '\n",
      "                                                                                                                              'and '\n",
      "                                                                                                                              'country, '\n",
      "                                                                                                                              'eg. '\n",
      "                                                                                                                              'San '\n",
      "                                                                                                                              'Francisco, '\n",
      "                                                                                                                              'USA',\n",
      "                                                                                                                  enum=None,\n",
      "                                                                                                                  items=None)},\n",
      "                                                                            type='object',\n",
      "                                                                            required=['location',\n",
      "                                                                                      'format'],\n",
      "                                                                            additionalProperties=None),\n",
      "                                                strict=False),\n",
      "                type='function')]\n",
      "tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'},\n",
      "               'strict': False},\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_experiment(\"chatmodel-quickstart\")\n",
    "# code_path = \"ollama_model.py\"\n",
    "# code_path = \"model.py\"\n",
    "# code_path = f\"{project_root}/src/training_tinyllama_for_tool_calling/services/agent.py\"\n",
    "# model = Model()\n",
    "\n",
    "# with mlflow.start_run(\n",
    "#     experiment_id=experiment.experiment_id,\n",
    "#     nested=True,\n",
    "# ):\n",
    "model_info = mlflow.pyfunc.log_model(\n",
    "    # \"ollama_model\",\n",
    "    \"model\",\n",
    "    python_model=code_path,\n",
    "    # python_model=model, # AttributeError: Can't get attribute 'unsloth_push_to_hub' on <module 'unsloth.save'\n",
    "    input_example={\n",
    "        # \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "        \"messages\": test_example_messages[0:1],\n",
    "        \"tools\": test_example_tools,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "057fcaee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.503142Z",
     "iopub.status.busy": "2024-11-21T04:44:38.502398Z",
     "iopub.status.idle": "2024-11-21T04:44:38.508157Z",
     "shell.execute_reply": "2024-11-21T04:44:38.507264Z"
    },
    "papermill": {
     "duration": 0.061655,
     "end_time": "2024-11-21T04:44:38.509948",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.448293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs:/476829bc4c43418f8f7b232d05179b4a/model'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93cd653e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.595779Z",
     "iopub.status.busy": "2024-11-21T04:44:38.595439Z",
     "iopub.status.idle": "2024-11-21T04:44:38.602130Z",
     "shell.execute_reply": "2024-11-21T04:44:38.601006Z"
    },
    "papermill": {
     "duration": 0.05117,
     "end_time": "2024-11-21T04:44:38.603779",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.552609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# result = loaded_model.predict(\n",
    "#     data={\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}],\n",
    "#         \"max_tokens\": 25,\n",
    "#     }\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "590870e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.691754Z",
     "iopub.status.busy": "2024-11-21T04:44:38.691471Z",
     "iopub.status.idle": "2024-11-21T04:44:38.695015Z",
     "shell.execute_reply": "2024-11-21T04:44:38.694324Z"
    },
    "papermill": {
     "duration": 0.052876,
     "end_time": "2024-11-21T04:44:38.697946",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.645070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat_response = chat_completion_request(\n",
    "#     documents=test_example_documents,\n",
    "#     messages=test_example_messages[0:1],\n",
    "#     tools=test_example_tools,\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1f08907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.787527Z",
     "iopub.status.busy": "2024-11-21T04:44:38.787237Z",
     "iopub.status.idle": "2024-11-21T04:44:38.790676Z",
     "shell.execute_reply": "2024-11-21T04:44:38.789965Z"
    },
    "papermill": {
     "duration": 0.05216,
     "end_time": "2024-11-21T04:44:38.792479",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.740319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat_response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fff079b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.889248Z",
     "iopub.status.busy": "2024-11-21T04:44:38.888931Z",
     "iopub.status.idle": "2024-11-21T04:44:38.892355Z",
     "shell.execute_reply": "2024-11-21T04:44:38.891512Z"
    },
    "papermill": {
     "duration": 0.05229,
     "end_time": "2024-11-21T04:44:38.894284",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.841994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "# print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ec8966e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:38.982321Z",
     "iopub.status.busy": "2024-11-21T04:44:38.981804Z",
     "iopub.status.idle": "2024-11-21T04:44:38.987022Z",
     "shell.execute_reply": "2024-11-21T04:44:38.985898Z"
    },
    "papermill": {
     "duration": 0.050775,
     "end_time": "2024-11-21T04:44:38.988811",
     "exception": false,
     "start_time": "2024-11-21T04:44:38.938036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# model.save_pretrained_merged(\n",
    "#     \"data/06_models/model/finetuned\",\n",
    "#     save_method=\"merged_16bit\",\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98fdb0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:39.075663Z",
     "iopub.status.busy": "2024-11-21T04:44:39.075381Z",
     "iopub.status.idle": "2024-11-21T04:44:39.079273Z",
     "shell.execute_reply": "2024-11-21T04:44:39.078393Z"
    },
    "papermill": {
     "duration": 0.052166,
     "end_time": "2024-11-21T04:44:39.082479",
     "exception": false,
     "start_time": "2024-11-21T04:44:39.030313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.paddding_side = \"right\"\n",
    "\n",
    "# # Get the ID of the MLflow Run that was automatically created above\n",
    "# # last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "# # Save a tokenizer without padding because it is only needed for training\n",
    "# # tokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n",
    "\n",
    "# with mlflow.start_run(\n",
    "#     nested=True,\n",
    "#     # run_id=last_run_id,\n",
    "# ):\n",
    "#     mlflow.transformers.log_model(\n",
    "#         # transformers_model={\"model\": sft_trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "#         transformers_model={\"model\": sft_trainer.model, \"tokenizer\": sft_trainer.tokenizer},\n",
    "#         # prompt_template=prompt_template,\n",
    "#         # signature=signature,\n",
    "#         artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53cf9115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:39.170689Z",
     "iopub.status.busy": "2024-11-21T04:44:39.170273Z",
     "iopub.status.idle": "2024-11-21T04:44:39.174701Z",
     "shell.execute_reply": "2024-11-21T04:44:39.173852Z"
    },
    "papermill": {
     "duration": 0.051161,
     "end_time": "2024-11-21T04:44:39.176664",
     "exception": false,
     "start_time": "2024-11-21T04:44:39.125503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If running in a Jupyter or Databricks notebook cell, uncomment the following line:\n",
    "# %%writefile \"./basic.py\"\n",
    "\n",
    "# import pandas as pd\n",
    "# from typing import List, Dict\n",
    "# from mlflow.pyfunc import PythonModel\n",
    "# from mlflow.models import set_model\n",
    "\n",
    "\n",
    "# class BasicModel(PythonModel):\n",
    "#     def exponential(self, numbers):\n",
    "#         return {f\"{x}\": 2**x for x in numbers}\n",
    "\n",
    "#     def predict(self, context, model_input) -> Dict[str, float]:\n",
    "#         if isinstance(model_input, pd.DataFrame):\n",
    "#             model_input = model_input.to_dict()[0].values()\n",
    "#         return self.exponential(model_input)\n",
    "\n",
    "\n",
    "# # Specify which definition in this script represents the model instance\n",
    "# set_model(BasicModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0da5cebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T04:44:39.266067Z",
     "iopub.status.busy": "2024-11-21T04:44:39.263414Z",
     "iopub.status.idle": "2024-11-21T04:44:39.270319Z",
     "shell.execute_reply": "2024-11-21T04:44:39.269588Z"
    },
    "papermill": {
     "duration": 0.051614,
     "end_time": "2024-11-21T04:44:39.272167",
     "exception": false,
     "start_time": "2024-11-21T04:44:39.220553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# mlflow.set_experiment(\"Basic Model From Code\")\n",
    "\n",
    "# model_path = \"basic.py\"\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     model_info = mlflow.pyfunc.log_model(\n",
    "#         python_model=model_path,  # Define the model as the path to the script that was just saved\n",
    "#         artifact_path=\"arithemtic_model\",\n",
    "#         input_example=[42.0, 24.0],\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 309.734862,
   "end_time": "2024-11-21T04:44:42.259849",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/training-tinyllama-for-tool-calling.ipynb",
   "output_path": "notebooks/training-tinyllama-for-tool-calling.output.ipynb",
   "parameters": {},
   "start_time": "2024-11-21T04:39:32.524987",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "169d130cdbb04469a573495aa213cb9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_557e4146a8044c6db19323d38a94e34d",
       "max": 7.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a9e5ba114fd74dfa91cc728bdbaca523",
       "tabbable": null,
       "tooltip": null,
       "value": 7.0
      }
     },
     "25c124302c074003b0ea666885c4f3cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3469ab9a1cf3425b80f0cda27b277732": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f80162f3dfd14c9782cbcb3eaf2edbb8",
        "IPY_MODEL_169d130cdbb04469a573495aa213cb9a",
        "IPY_MODEL_c0e64dd8f8a14c19abba9af61d3e054a"
       ],
       "layout": "IPY_MODEL_25c124302c074003b0ea666885c4f3cd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "557e4146a8044c6db19323d38a94e34d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9e5ba114fd74dfa91cc728bdbaca523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b3adebec8410460f8e1167a2146dd3df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bacc03360ef14ece94ac2f0b9be094bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bed40f73b74b4164a9bb2a6305aef114": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c0e64dd8f8a14c19abba9af61d3e054a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_da8afe3a5ac74de9b8eebf9b761ebb71",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bacc03360ef14ece94ac2f0b9be094bf",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡7/7â€‡[00:00&lt;00:00,â€‡187.89it/s]"
      }
     },
     "da8afe3a5ac74de9b8eebf9b761ebb71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f80162f3dfd14c9782cbcb3eaf2edbb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b3adebec8410460f8e1167a2146dd3df",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bed40f73b74b4164a9bb2a6305aef114",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloadingâ€‡artifacts:â€‡100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}