{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2022787",
   "metadata": {
    "papermill": {
     "duration": 0.077272,
     "end_time": "2024-11-20T03:00:24.182072",
     "exception": false,
     "start_time": "2024-11-20T03:00:24.104800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training TinyLlama for Tool-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfc08b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:24.250224Z",
     "iopub.status.busy": "2024-11-20T03:00:24.249794Z",
     "iopub.status.idle": "2024-11-20T03:00:24.260883Z",
     "shell.execute_reply": "2024-11-20T03:00:24.260074Z"
    },
    "papermill": {
     "duration": 0.051511,
     "end_time": "2024-11-20T03:00:24.263140",
     "exception": false,
     "start_time": "2024-11-20T03:00:24.211629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/mjschock/Projects/training-tinyllama-for-tool-calling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_root = os.getcwd()\n",
    "\n",
    "while not os.path.exists(os.path.join(project_root, \"register_prefect_flow.py\")):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d818ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:24.332564Z",
     "iopub.status.busy": "2024-11-20T03:00:24.332273Z",
     "iopub.status.idle": "2024-11-20T03:00:31.173671Z",
     "shell.execute_reply": "2024-11-20T03:00:31.170258Z"
    },
    "papermill": {
     "duration": 6.87663,
     "end_time": "2024-11-20T03:00:31.179191",
     "exception": false,
     "start_time": "2024-11-20T03:00:24.302561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r https://raw.githubusercontent.com/mjschock/training-tinyllama-for-tool-calling/refs/heads/main/requirements.txt\n",
    "%pip install -qr $project_root/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce6c208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:31.275550Z",
     "iopub.status.busy": "2024-11-20T03:00:31.275101Z",
     "iopub.status.idle": "2024-11-20T03:00:39.900661Z",
     "shell.execute_reply": "2024-11-20T03:00:39.899402Z"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "5eff0d61-05b4-471c-eea2-c2e84a915109",
    "papermill": {
     "duration": 8.663919,
     "end_time": "2024-11-20T03:00:39.902115",
     "exception": false,
     "start_time": "2024-11-20T03:00:31.238196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import mlflow\n",
    "from mlflow.types.llm import (\n",
    "    ChatChoice,\n",
    "    ChatMessage,\n",
    "    ChatResponse,\n",
    "    FunctionToolCallArguments,\n",
    "    ToolCall,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcef9ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:39.980680Z",
     "iopub.status.busy": "2024-11-20T03:00:39.980159Z",
     "iopub.status.idle": "2024-11-20T03:00:39.998766Z",
     "shell.execute_reply": "2024-11-20T03:00:39.997499Z"
    },
    "papermill": {
     "duration": 0.068185,
     "end_time": "2024-11-20T03:00:40.000378",
     "exception": false,
     "start_time": "2024-11-20T03:00:39.932193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_id: 459047286874428176\n",
      "Artifact Location: file:///home/mjschock/Projects/training-tinyllama-for-tool-calling/mlruns/459047286874428176\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri('/my/custom/path')\n",
    "mlflow.set_tracking_uri(f\"file://{project_root}/mlruns\")\n",
    "\n",
    "# Set an experiment name, which must be unique and case-sensitive.\n",
    "experiment = mlflow.set_experiment(\"Training TinyLlama for Tool-calling\")\n",
    "# Get Experiment Details\n",
    "print(f\"Experiment_id: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "print(f\"Tags: {experiment.tags}\")\n",
    "print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a777d7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:40.094090Z",
     "iopub.status.busy": "2024-11-20T03:00:40.093320Z",
     "iopub.status.idle": "2024-11-20T03:00:40.100800Z",
     "shell.execute_reply": "2024-11-20T03:00:40.099317Z"
    },
    "papermill": {
     "duration": 0.064101,
     "end_time": "2024-11-20T03:00:40.103106",
     "exception": false,
     "start_time": "2024-11-20T03:00:40.039005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "if not os.path.exists(os.path.join(project_root, \"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\")):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "    # model.save_pretrained(\"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    model.save_pretrained(os.path.join(project_root, \"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"))\n",
    "    # tokenizer.save_pretrained(\"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    tokenizer.save_pretrained(os.path.join(project_root, \"data/06_models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"))\n",
    "\n",
    "    del model\n",
    "    del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91e5618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:40.187191Z",
     "iopub.status.busy": "2024-11-20T03:00:40.186316Z",
     "iopub.status.idle": "2024-11-20T03:00:46.901385Z",
     "shell.execute_reply": "2024-11-20T03:00:46.899760Z"
    },
    "papermill": {
     "duration": 6.763147,
     "end_time": "2024-11-20T03:00:46.903158",
     "exception": false,
     "start_time": "2024-11-20T03:00:40.140011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/tinyllama-chat-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "max_seq_length = 4096\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    max_seq_length=max_seq_length,\n",
    "    model_name=model_name,\n",
    ")\n",
    "\n",
    "# Save a tokenizer without padding because it is only needed for training\n",
    "# tokenizer_no_pad = AutoTokenizer.from_pretrained(model_name, add_bos_token=True) # https://mlflow.org/docs/latest/llms/transformers/tutorials/fine-tuning/transformers-peft.html#Save-the-PEFT-Model-to-MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248f8dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:46.975009Z",
     "iopub.status.busy": "2024-11-20T03:00:46.974323Z",
     "iopub.status.idle": "2024-11-20T03:00:46.979230Z",
     "shell.execute_reply": "2024-11-20T03:00:46.978422Z"
    },
    "papermill": {
     "duration": 0.049809,
     "end_time": "2024-11-20T03:00:46.982157",
     "exception": false,
     "start_time": "2024-11-20T03:00:46.932348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd046e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:47.157633Z",
     "iopub.status.busy": "2024-11-20T03:00:47.156793Z",
     "iopub.status.idle": "2024-11-20T03:00:47.175719Z",
     "shell.execute_reply": "2024-11-20T03:00:47.172057Z"
    },
    "papermill": {
     "duration": 0.087111,
     "end_time": "2024-11-20T03:00:47.178292",
     "exception": false,
     "start_time": "2024-11-20T03:00:47.091181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_MESSAGE = {\n",
    "    \"content\": \"You are an AI agent acting as a human assistant.\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "tools_template = \"\"\"\n",
    "{\n",
    "  \"tools\": [\n",
    "  {% for tool in tools %}\n",
    "    {\n",
    "      \"function\": {\n",
    "        \"description\": \"{{ tool.function.description }}\",\n",
    "        \"name\": \"{{ tool.function.name }}\",\n",
    "        \"parameters\": {{ tool.function.parameters | tojson }}\n",
    "      },\n",
    "      \"type\": \"{{ tool.type }}\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\n",
    "If you would like to suggest one or more tool calls, please respond in the following format:\n",
    "{\n",
    "  \"finish_reason\": \"tool_calls\",\n",
    "  \"tool_calls\": [\n",
    "  {% for tool in tools %}\n",
    "    {\n",
    "      \"arguments\": \"{\\\\\"parameter_name\\\\\": \\\\\"parameter_value\\\\\"}\",\n",
    "      \"id\": \"call_id\",\n",
    "      \"name\": \"tool_name\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tool_calls_template = \"\"\"\n",
    "{\n",
    "  \"finish_reason\": \"tool_calls\",\n",
    "  \"tool_calls\": [\n",
    "  {% for tool_call in message.tool_calls %}\n",
    "    {\n",
    "      \"arguments\": {{ tool_call.function.arguments | tojson }},\n",
    "      \"id\": \"{{ tool_call.id }}\",\n",
    "      \"name\": \"{{ tool_call.function.name }}\"\n",
    "    }{% if not loop.last %},{% endif %}\\n\n",
    "  {% endfor %}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tool_response_template = \"\"\"\n",
    "{\n",
    "  \"content\": {{ message.content | tojson }},\n",
    "  \"name\": \"{{ message.name }}\",\n",
    "  \"tool_call_id\": \"{{ message.tool_call_id }}\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "if model_name == \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\":\n",
    "  start_header_id = \"<|\"\n",
    "  end_header_id = \"|>\"\n",
    "\n",
    "else:\n",
    "  start_header_id = \"<|start_header_id|>\"\n",
    "  end_header_id = \"<|end_header_id|>\"\n",
    "\n",
    "role_header_template = start_header_id + \"{{ message.role }}\" + end_header_id + \"{{ '\\n' }}\"\n",
    "assistant_generation_role_header_template = start_header_id + \"assistant\" + end_header_id + \"{{ '\\n' }}\"\n",
    "\n",
    "# Influenced by:\n",
    "# https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models\n",
    "# https://docs.anthropic.com/en/docs/build-with-claude/tool-use\n",
    "# https://github.com/abetlen/llama-cpp-python/blob/7c4aead82d349469bbbe7d8c0f4678825873c039/llama_cpp/llama_chat_format.py#L3387\n",
    "# https://github.com/Mozilla-Ocho/llamafile/blob/66a84d8aea2990895fc4f64786406fea64e79197/llama.cpp/server/server.cpp#L480 (need <|im_start|> b/c Mozilla)\n",
    "# https://github.com/openai/openai-python/blob/120d225b91a8453e15240a49fb1c6794d8119326/chatml.md\n",
    "# https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html#prompt\n",
    "# https://huggingface.co/blog/unified-tool-use\n",
    "chat_template = (\n",
    "    # Configuration and defaults\n",
    "    \"{%- set config = namespace(has_system_message=false, has_tools=false) -%}\"\n",
    "    \"{%- set system_messages = messages | selectattr('role', 'equalto', 'system') | list -%}\"\n",
    "    \"{%- set config.has_system_message = system_messages | length > 0 -%}\"\n",
    "    \"{%- set config.has_tools = tools is defined and tools | length > 0 -%}\"\n",
    "\n",
    "    # Ensure system message exists\n",
    "    \"{%- if not config.has_system_message -%}\"\n",
    "    f'{{%- set messages = [{{ \"content\": \"{DEFAULT_SYSTEM_MESSAGE[\"content\"]}\", \"role\": \"{DEFAULT_SYSTEM_MESSAGE[\"role\"]}\" }}] + messages -%}}'\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Process messages\n",
    "    \"{%- for message in messages -%}\"\n",
    "    # \"<|{{ message.role }}|>{{ '\\n' }}\" # \"<|start_header_id|>{{ message.role }}<|end_header_id|>{{ '\\n' }}\"\n",
    "    # f\"{start_header_id}{{ message.role }}{end_header_id}{{ '\\n' }}\"\n",
    "    # start_header_id + \"{{ message.role }}\" + end_header_id + \"{{ '\\n' }}\"\n",
    "    # TODO: add bos_token if first message?\n",
    "    \"{% if loop.first %}{{ bos_token }}{% endif %}\"f\"{role_header_template}\"\n",
    "\n",
    "    # System message handling\n",
    "    \"{%- if message.role == 'system' -%}\"\n",
    "    \"{{ message.content }}\"\n",
    "    \"{%- if config.has_tools -%}\"\n",
    "    \"{{ '\\n\\n' }}You are aware of the following tools in your environment:\"\n",
    "    f\"{tools_template}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\" # <|eot_id|>\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # User message handling\n",
    "    \"{%- if message.role == 'user' -%}\"\n",
    "    \"{{ message.content }}{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Assistant message handling\n",
    "    \"{%- if message.role == 'assistant' -%}\"\n",
    "    \"{% generation %}\"\n",
    "    \"{%- if message.tool_calls | default(false) -%}\"\n",
    "    f\"{tool_calls_template}\"\n",
    "    \"{%- else -%}\"\n",
    "    \"{{ message.content }}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{% endgeneration %}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "\n",
    "    # Tool message handling\n",
    "    \"{%- if message.role == 'tool' -%}\"\n",
    "    f\"{tool_response_template}\"\n",
    "    \"{{ eos_token }}{{ '\\n' }}\"\n",
    "    \"{%- endif -%}\"\n",
    "    \"{%- endfor -%}\"\n",
    "\n",
    "    # Generation prompt\n",
    "    \"{%- if add_generation_prompt -%}\"\n",
    "    # \"<|assistant|>{{ '\\n' }}\" # <|start_header_id|>assistant<|end_header_id|>\n",
    "    f\"{assistant_generation_role_header_template}\"\n",
    "    \"{%- endif -%}\"\n",
    ")\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template=(\n",
    "#         chat_template,\n",
    "#         \"eos_token\"\n",
    "#     ),\n",
    "#     map_eos_token=True,\n",
    "# )\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18740d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:47.298060Z",
     "iopub.status.busy": "2024-11-20T03:00:47.297513Z",
     "iopub.status.idle": "2024-11-20T03:00:47.311247Z",
     "shell.execute_reply": "2024-11-20T03:00:47.309946Z"
    },
    "papermill": {
     "duration": 0.072376,
     "end_time": "2024-11-20T03:00:47.313831",
     "exception": false,
     "start_time": "2024-11-20T03:00:47.241455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert model.config.bos_token_id == tokenizer.bos_token_id, f\"{model.config.bos_token_id} != {tokenizer.bos_token_id}\"\n",
    "\n",
    "try:\n",
    "    assert model.config.eos_token_id == tokenizer.eos_token_id, f\"{model.config.eos_token_id} != {tokenizer.eos_token_id}\"\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, f\"{model.config.pad_token_id} != {tokenizer.pad_token_id}\"\n",
    "\n",
    "assert model.generation_config.bos_token_id == tokenizer.bos_token_id, f\"{model.generation_config.bos_token_id} != {tokenizer.bos_token_id}\"\n",
    "\n",
    "try:\n",
    "    assert model.generation_config.eos_token_id == tokenizer.eos_token_id, f\"{model.generation_config.eos_token_id} != {tokenizer.eos_token_id}\"\n",
    "\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "assert model.generation_config.pad_token_id == tokenizer.pad_token_id, f\"{model.generation_config.pad_token_id} != {tokenizer.pad_token_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2279008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:47.477126Z",
     "iopub.status.busy": "2024-11-20T03:00:47.476312Z",
     "iopub.status.idle": "2024-11-20T03:00:47.482431Z",
     "shell.execute_reply": "2024-11-20T03:00:47.481147Z"
    },
    "papermill": {
     "duration": 0.091947,
     "end_time": "2024-11-20T03:00:47.485118",
     "exception": false,
     "start_time": "2024-11-20T03:00:47.393171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2a599c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:00:47.613801Z",
     "iopub.status.busy": "2024-11-20T03:00:47.613208Z",
     "iopub.status.idle": "2024-11-20T03:01:13.526123Z",
     "shell.execute_reply": "2024-11-20T03:01:13.523579Z"
    },
    "papermill": {
     "duration": 25.991453,
     "end_time": "2024-11-20T03:01:13.538627",
     "exception": false,
     "start_time": "2024-11-20T03:00:47.547174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will use up to 0.0 out of 15.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                   | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                     | 13/22 [00:00<00:00, 129.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 161.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0_merged_16bit\"),\n",
    "    save_method=\"merged_16bit\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec45dcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:13.751134Z",
     "iopub.status.busy": "2024-11-20T03:01:13.750456Z",
     "iopub.status.idle": "2024-11-20T03:01:21.552137Z",
     "shell.execute_reply": "2024-11-20T03:01:21.551202Z"
    },
    "papermill": {
     "duration": 7.867072,
     "end_time": "2024-11-20T03:01:21.558609",
     "exception": false,
     "start_time": "2024-11-20T03:01:13.691537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': '[]',\n",
       " 'has_parallel_tool_calls': True,\n",
       " 'messages': '[{\"role\": \"user\", \"content\": \"What\\'s the weather like in San Francisco and New York?\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_0\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\\\"location\\\\\": \\\\\"San Francisco, USA\\\\\", \\\\\"format\\\\\": \\\\\"celsius\\\\\"}\"}}, {\"id\": \"call_1\", \"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"arguments\": \"{\\\\\"location\\\\\": \\\\\"New York, USA\\\\\", \\\\\"format\\\\\": \\\\\"celsius\\\\\"}\"}}]}, {\"role\": \"tool\", \"name\": \"get_current_weather\", \"tool_call_id\": \"call_0\", \"content\": \"21.0\"}, {\"role\": \"tool\", \"name\": \"get_current_weather\", \"tool_call_id\": \"call_1\", \"content\": \"18.5\"}, {\"role\": \"assistant\", \"content\": \"The current temperature in San Francisco is 21\\\\u00b0C (70\\\\u00b0F), while in New York it\\'s 18.5\\\\u00b0C (65\\\\u00b0F).\"}]',\n",
       " 'tools': '[{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}}}]'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"mjschock/chat_threads\", split=\"train\")\n",
    "validation_dataset = load_dataset(\"mjschock/chat_threads\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"mjschock/chat_threads\", split=\"test\")\n",
    "\n",
    "test_example = test_dataset[0]\n",
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1157909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:21.636246Z",
     "iopub.status.busy": "2024-11-20T03:01:21.635753Z",
     "iopub.status.idle": "2024-11-20T03:01:21.644252Z",
     "shell.execute_reply": "2024-11-20T03:01:21.643001Z"
    },
    "papermill": {
     "duration": 0.047922,
     "end_time": "2024-11-20T03:01:21.645722",
     "exception": false,
     "start_time": "2024-11-20T03:01:21.597800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_example_documents = json.loads(test_example[\"documents\"])\n",
    "test_example_messages = json.loads(test_example[\"messages\"])\n",
    "test_example_tools = json.loads(test_example[\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2b1cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:21.738640Z",
     "iopub.status.busy": "2024-11-20T03:01:21.737739Z",
     "iopub.status.idle": "2024-11-20T03:01:21.745508Z",
     "shell.execute_reply": "2024-11-20T03:01:21.744759Z"
    },
    "papermill": {
     "duration": 0.055422,
     "end_time": "2024-11-20T03:01:21.746940",
     "exception": false,
     "start_time": "2024-11-20T03:01:21.691518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_example_messages:\n",
      "[{'content': \"What's the weather like in San Francisco and New York?\",\n",
      "  'role': 'user'},\n",
      " {'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': '{\"location\": \"San Francisco, '\n",
      "                                            'USA\", \"format\": \"celsius\"}',\n",
      "                               'name': 'get_current_weather'},\n",
      "                  'id': 'call_0',\n",
      "                  'type': 'function'},\n",
      "                 {'function': {'arguments': '{\"location\": \"New York, USA\", '\n",
      "                                            '\"format\": \"celsius\"}',\n",
      "                               'name': 'get_current_weather'},\n",
      "                  'id': 'call_1',\n",
      "                  'type': 'function'}]},\n",
      " {'content': '21.0',\n",
      "  'name': 'get_current_weather',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_0'},\n",
      " {'content': '18.5',\n",
      "  'name': 'get_current_weather',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_1'},\n",
      " {'content': 'The current temperature in San Francisco is 21Â°C (70Â°F), while '\n",
      "             \"in New York it's 18.5Â°C (65Â°F).\",\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print('test_example_messages:')\n",
    "pprint(test_example_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7f8d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:21.831036Z",
     "iopub.status.busy": "2024-11-20T03:01:21.830665Z",
     "iopub.status.idle": "2024-11-20T03:01:21.836692Z",
     "shell.execute_reply": "2024-11-20T03:01:21.835754Z"
    },
    "papermill": {
     "duration": 0.057718,
     "end_time": "2024-11-20T03:01:21.838346",
     "exception": false,
     "start_time": "2024-11-20T03:01:21.780628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_example_tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'}},\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "print('test_example_tools:')\n",
    "pprint(test_example_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415f20ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:21.904767Z",
     "iopub.status.busy": "2024-11-20T03:01:21.904433Z",
     "iopub.status.idle": "2024-11-20T03:01:21.945826Z",
     "shell.execute_reply": "2024-11-20T03:01:21.944816Z"
    },
    "papermill": {
     "duration": 0.076759,
     "end_time": "2024-11-20T03:01:21.947370",
     "exception": false,
     "start_time": "2024-11-20T03:01:21.870611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=True,\n",
    "    conversation=test_example_messages[0:1], # Only the user message, note that the system message will automatically be added\n",
    "    documents=test_example_documents,\n",
    "    tools=test_example_tools,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "prompt_and_response = tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=False,\n",
    "    conversation=test_example_messages[0:2], # Only the user and first assistant message, note that the system message will automatically be added\n",
    "    documents=test_example_documents,\n",
    "    tools=test_example_tools,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "response = prompt_and_response.replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa26b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:22.018173Z",
     "iopub.status.busy": "2024-11-20T03:01:22.017875Z",
     "iopub.status.idle": "2024-11-20T03:01:22.025030Z",
     "shell.execute_reply": "2024-11-20T03:01:22.023544Z"
    },
    "papermill": {
     "duration": 0.047627,
     "end_time": "2024-11-20T03:01:22.026871",
     "exception": false,
     "start_time": "2024-11-20T03:01:21.979244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s><|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s>\n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b154ebfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:22.140869Z",
     "iopub.status.busy": "2024-11-20T03:01:22.139230Z",
     "iopub.status.idle": "2024-11-20T03:01:22.146917Z",
     "shell.execute_reply": "2024-11-20T03:01:22.145842Z"
    },
    "papermill": {
     "duration": 0.060931,
     "end_time": "2024-11-20T03:01:22.148925",
     "exception": false,
     "start_time": "2024-11-20T03:01:22.087994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (ground truth):\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\",\n",
      "      \"id\": \"call_0\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    },\n",
      "    {\n",
      "      \"arguments\": \"{\\\"location\\\": \\\"New York, USA\\\", \\\"format\\\": \\\"celsius\\\"}\",\n",
      "      \"id\": \"call_1\",\n",
      "      \"name\": \"get_current_weather\"\n",
      "    }\n",
      "  ]\n",
      "}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"response (ground truth):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83be9db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:22.295179Z",
     "iopub.status.busy": "2024-11-20T03:01:22.291514Z",
     "iopub.status.idle": "2024-11-20T03:01:23.015575Z",
     "shell.execute_reply": "2024-11-20T03:01:23.014375Z"
    },
    "papermill": {
     "duration": 0.821387,
     "end_time": "2024-11-20T03:01:23.017929",
     "exception": false,
     "start_time": "2024-11-20T03:01:22.196542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset for training.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to preprocess\n",
    "        tokenizer: Tokenizer to use for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: Preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Extract the messages from the example\n",
    "        conversation = examples[\"messages\"]\n",
    "        documents = examples.get(\"documents\", [])\n",
    "        tools = examples.get(\"tools\", [])\n",
    "\n",
    "        # Apply chat template to generate tokenized input and assistant mask\n",
    "        tokenized_output = tokenizer.apply_chat_template(\n",
    "            add_generation_prompt=False,\n",
    "            conversation=json.loads(conversation),\n",
    "            documents=json.loads(documents),\n",
    "            max_length=max_seq_length,\n",
    "            padding=\"longest\",\n",
    "            return_assistant_tokens_mask=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            tokenize=True,\n",
    "            tools=json.loads(tools),\n",
    "            truncation=True,  # TODO: verify we're not truncating anything in the datasets\n",
    "        )\n",
    "\n",
    "        # Extract the input IDs and assistant tokens mask\n",
    "        input_ids = tokenized_output[\"input_ids\"][0]\n",
    "        assistant_masks = torch.tensor(tokenized_output[\"assistant_masks\"])\n",
    "        attention_mask = tokenized_output[\"attention_mask\"][0]\n",
    "\n",
    "        # Use the assistant mask to create labels\n",
    "        labels = torch.where(assistant_masks == 1, input_ids, torch.tensor(-100))\n",
    "\n",
    "        return {\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=False,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )  # TODO: use batched=True\n",
    "\n",
    "\n",
    "tokenized_train_dataset = load_and_preprocess_data(\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "tokenized_validation_dataset = load_and_preprocess_data(\n",
    "    validation_dataset,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = load_and_preprocess_data(\n",
    "    test_dataset,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "545fd3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:23.182324Z",
     "iopub.status.busy": "2024-11-20T03:01:23.181728Z",
     "iopub.status.idle": "2024-11-20T03:01:23.196957Z",
     "shell.execute_reply": "2024-11-20T03:01:23.193775Z"
    },
    "papermill": {
     "duration": 0.096234,
     "end_time": "2024-11-20T03:01:23.199969",
     "exception": false,
     "start_time": "2024-11-20T03:01:23.103735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_test_prediction():\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=True,\n",
    "        conversation=test_example_messages[0:1], # Only the user message, note that the system message will automatically be added\n",
    "        documents=test_example_documents,\n",
    "        tools=test_example_tools,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        do_sample=False,\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True,\n",
    "        # temperature=0.0,\n",
    "    )\n",
    "\n",
    "    batch_decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    prompt = batch_decoded_outputs[0][0:len(tokenizer.decode(inputs[0]))]\n",
    "    response = batch_decoded_outputs[0][len(tokenizer.decode(inputs[0])):]\n",
    "\n",
    "    return prompt, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01537ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:23.354953Z",
     "iopub.status.busy": "2024-11-20T03:01:23.354072Z",
     "iopub.status.idle": "2024-11-20T03:01:32.868651Z",
     "shell.execute_reply": "2024-11-20T03:01:32.867734Z"
    },
    "papermill": {
     "duration": 9.599534,
     "end_time": "2024-11-20T03:01:32.870535",
     "exception": false,
     "start_time": "2024-11-20T03:01:23.271001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "prompt, response = generate_test_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e862f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:32.941423Z",
     "iopub.status.busy": "2024-11-20T03:01:32.940789Z",
     "iopub.status.idle": "2024-11-20T03:01:32.948533Z",
     "shell.execute_reply": "2024-11-20T03:01:32.946546Z"
    },
    "papermill": {
     "duration": 0.05012,
     "end_time": "2024-11-20T03:01:32.951712",
     "exception": false,
     "start_time": "2024-11-20T03:01:32.901592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dd39c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:33.031614Z",
     "iopub.status.busy": "2024-11-20T03:01:33.031006Z",
     "iopub.status.idle": "2024-11-20T03:01:33.035126Z",
     "shell.execute_reply": "2024-11-20T03:01:33.034498Z"
    },
    "papermill": {
     "duration": 0.044284,
     "end_time": "2024-11-20T03:01:33.036334",
     "exception": false,
     "start_time": "2024-11-20T03:01:32.992050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s> <|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s> \n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c6d845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:33.117993Z",
     "iopub.status.busy": "2024-11-20T03:01:33.117368Z",
     "iopub.status.idle": "2024-11-20T03:01:33.122316Z",
     "shell.execute_reply": "2024-11-20T03:01:33.121394Z"
    },
    "papermill": {
     "duration": 0.053401,
     "end_time": "2024-11-20T03:01:33.123713",
     "exception": false,
     "start_time": "2024-11-20T03:01:33.070312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (predicted, before training):\n",
      "I don't have access to real-time weather data, but according to the information available data, the weather in san francisco and new york are both hot and humid. San francisco has a maximum temperature of 328.8Â°c (992.8f) and minimum of 25.8Â°c (439.2f, while new york has maximum of 39.8Â° (434.2f and minimum 2.8Â° (32f).</s>\n"
     ]
    }
   ],
   "source": [
    "print('response (predicted, before training):')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0903b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:33.191052Z",
     "iopub.status.busy": "2024-11-20T03:01:33.190601Z",
     "iopub.status.idle": "2024-11-20T03:01:36.679706Z",
     "shell.execute_reply": "2024-11-20T03:01:36.678404Z"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "b630cc80-ff95-45a2-cc0d-38666010d73b",
    "papermill": {
     "duration": 3.524199,
     "end_time": "2024-11-20T03:01:36.681782",
     "exception": false,
     "start_time": "2024-11-20T03:01:33.157583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjschock/Projects/training-tinyllama-for-tool-calling/.venv/lib/python3.10/site-packages/unsloth/models/_utils.py:697: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    bias=\"none\",\n",
    "    loftq_config=None,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    model=model,\n",
    "    r=16,\n",
    "    random_state=42,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    use_rslora=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72568622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:36.785267Z",
     "iopub.status.busy": "2024-11-20T03:01:36.784943Z",
     "iopub.status.idle": "2024-11-20T03:01:36.790034Z",
     "shell.execute_reply": "2024-11-20T03:01:36.789022Z"
    },
    "papermill": {
     "duration": 0.063895,
     "end_time": "2024-11-20T03:01:36.792362",
     "exception": false,
     "start_time": "2024-11-20T03:01:36.728467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82c5c384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:36.909574Z",
     "iopub.status.busy": "2024-11-20T03:01:36.909080Z",
     "iopub.status.idle": "2024-11-20T03:01:45.144290Z",
     "shell.execute_reply": "2024-11-20T03:01:45.143042Z"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4b809e6d-271f-446f-dec8-abe0d13259f8",
    "papermill": {
     "duration": 8.298505,
     "end_time": "2024-11-20T03:01:45.146378",
     "exception": false,
     "start_time": "2024-11-20T03:01:36.847873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mjschock/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate.combine([\"bleu\", \"meteor\", \"rouge\"])\n",
    "metrics_tracker = {}\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction, compute_result: bool) -> Dict:\n",
    "    assert isinstance(\n",
    "        eval_pred, EvalPrediction\n",
    "    ), f\"Expected EvalPrediction, got {type(eval_pred)}\"\n",
    "\n",
    "    all_labels = eval_pred.label_ids\n",
    "    all_preds = eval_pred.predictions\n",
    "    is_last_step = compute_result\n",
    "\n",
    "    all_labels[all_labels == -100] = tokenizer.pad_token_id\n",
    "    references: List[str] = tokenizer.batch_decode(\n",
    "        all_labels, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        all_preds.shape == all_labels.shape\n",
    "    ), f\"Expected predictions and labels to have the same shape, got {all_preds.shape} and {all_labels.shape}\"\n",
    "\n",
    "    predictions: List[str] = tokenizer.batch_decode(\n",
    "        all_preds, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    assert len(predictions) == len(\n",
    "        references\n",
    "    ), f\"Expected predictions and references to have the same length, got {len(predictions)} and {len(references)}\"\n",
    "\n",
    "    eval_batch_metrics = metrics.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "    computed_metrics = {}\n",
    "\n",
    "    for key, value in eval_batch_metrics.items():\n",
    "        if type(value) in [list, np.ndarray]:\n",
    "            value = np.mean(value)\n",
    "\n",
    "        metrics_tracker[key] = np.mean([metrics_tracker.get(key, 0.0), value])\n",
    "        computed_metrics[key] = metrics_tracker[key]\n",
    "\n",
    "        if is_last_step:\n",
    "            metrics_tracker[key] = 0.0\n",
    "\n",
    "    return computed_metrics\n",
    "\n",
    "def preprocess_logits_for_metrics(\n",
    "    logits: torch.Tensor, labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return pred_ids\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    args=SFTConfig(\n",
    "        # auto_find_batch_size=True,\n",
    "        # batch_eval_metrics=True,\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        # eval_accumulation_steps=16,\n",
    "        # eval_on_start=True,\n",
    "        # eval_steps=1.0,\n",
    "        # eval_strategy=\"epoch\",\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        # gradient_accumulation_steps=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # gradient_checkpointing=\"unsloth\",\n",
    "        # learning_rate=5e-05,\n",
    "        learning_rate=2e-4,\n",
    "        # load_best_model_at_end=True,\n",
    "        logging_steps=1.0,\n",
    "        # logging_strategy=\"steps\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # max_seq_length=max_seq_length,\n",
    "        # max_steps=60,\n",
    "        # max_steps=3,\n",
    "        max_steps=1,\n",
    "        # num_of_sequences=1,\n",
    "        # num_train_epochs=3.0,\n",
    "        # num_train_epochs=1.0,\n",
    "        optim=\"adamw_8bit\",\n",
    "        # output_dir=\"outputs\",\n",
    "        # output_dir=\"data/06_models/model\",\n",
    "        # output_dir=\"data/06_models/model/checkpoints\",\n",
    "        # output_dir=os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0_merged_16bit/checkpoints\"),\n",
    "        output_dir=os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/checkpoints\"),\n",
    "        overwrite_output_dir=True,\n",
    "        # packing=False,\n",
    "        per_device_eval_batch_size=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        # push_to_hub=False,\n",
    "        report_to=\"mlflow\",\n",
    "        # save_steps=1.0,\n",
    "        # save_strategy=\"epoch\",\n",
    "        # save_total_limit=1,\n",
    "        seed=42,\n",
    "        warmup_steps=5,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer),\n",
    "    dataset_num_proc=1,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    model=model,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    ")\n",
    "\n",
    "# dpo_trainer = DPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5aa642a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:45.236127Z",
     "iopub.status.busy": "2024-11-20T03:01:45.234776Z",
     "iopub.status.idle": "2024-11-20T03:01:45.240856Z",
     "shell.execute_reply": "2024-11-20T03:01:45.239736Z"
    },
    "papermill": {
     "duration": 0.056455,
     "end_time": "2024-11-20T03:01:45.242610",
     "exception": false,
     "start_time": "2024-11-20T03:01:45.186155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34449d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:45.315106Z",
     "iopub.status.busy": "2024-11-20T03:01:45.314480Z",
     "iopub.status.idle": "2024-11-20T03:01:45.333104Z",
     "shell.execute_reply": "2024-11-20T03:01:45.332411Z"
    },
    "papermill": {
     "duration": 0.055391,
     "end_time": "2024-11-20T03:01:45.334410",
     "exception": false,
     "start_time": "2024-11-20T03:01:45.279019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <|system|>\\nYou are an AI agent acting as a human assistant.\\n\\nYou are aware of the following tools in your environment:\\n{\\n  \"tools\": [\\n    {\\n      \"function\": {\\n        \"description\": \"Calculates the electrostatic potential energy.\",\\n        \"name\": \"calculate_electrostatic_potential_energy\",\\n        \"parameters\": {\"properties\": {\"charge\": {\"type\": \"number\", \"description\": \"The charge of the object, in coulombs.\"}, \"voltage\": {\"type\": \"number\", \"description\": \"The voltage of the object, in volts.\"}}, \"type\": \"object\", \"required\": [\"charge\", \"voltage\"]}\\n      },\\n      \"type\": \"function\"\\n    }\\n  ]\\n}\\n\\nIf you would like to suggest one or more tool calls, please respond in the following format:\\n{\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"parameter_name\\\\\": \\\\\"parameter_value\\\\\"}\",\\n      \"id\": \"call_id\",\\n      \"name\": \"tool_name\"\\n    }\\n  ]\\n}</s> \\n<|user|>\\nI\\'m working on a physics simulation and I have a micro-particle here charged at 7.8 coulombs. It\\'s placed in an electromagnetic field with a voltage of 15.2 volts. Can you calculate the electrostatic potential energy for this particle in the given field?</s> \\n<|assistant|>\\n{\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"charge\\\\\": 7.8, \\\\\"voltage\\\\\": 15.2}\",\\n      \"id\": \"call_0\",\\n      \"name\": \"calculate_electrostatic_potential_energy\"\\n    }\\n  ]\\n}</s> \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first training example. When we decod the input_ids, we'll see the full chat history.\n",
    "\n",
    "tokenizer.decode(sft_trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4de089c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:45.414089Z",
     "iopub.status.busy": "2024-11-20T03:01:45.413767Z",
     "iopub.status.idle": "2024-11-20T03:01:45.426693Z",
     "shell.execute_reply": "2024-11-20T03:01:45.425696Z"
    },
    "papermill": {
     "duration": 0.060172,
     "end_time": "2024-11-20T03:01:45.428461",
     "exception": false,
     "start_time": "2024-11-20T03:01:45.368289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {\\n  \"finish_reason\": \"tool_calls\",\\n  \"tool_calls\": [\\n    {\\n      \"arguments\": \"{\\\\\"charge\\\\\": 7.8, \\\\\"voltage\\\\\": 15.2}\",\\n      \"id\": \"call_0\",\\n      \"name\": \"calculate_electrostatic_potential_energy\"\\n    }\\n  ]\\n}      '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's take a look at the labels. The labels are the same as the input_ids, except that the assistant tokens are replaced with -100. This is because we want to predict the assistant tokens.\n",
    "\n",
    "space = tokenizer(\" \", add_special_tokens=False).input_ids[0]\n",
    "tokenizer.decode(\n",
    "    [space if x == -100 else x for x in sft_trainer.train_dataset[0][\"labels\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e44228da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:45.522865Z",
     "iopub.status.busy": "2024-11-20T03:01:45.522315Z",
     "iopub.status.idle": "2024-11-20T03:01:45.530058Z",
     "shell.execute_reply": "2024-11-20T03:01:45.528987Z"
    },
    "papermill": {
     "duration": 0.06594,
     "end_time": "2024-11-20T03:01:45.531779",
     "exception": false,
     "start_time": "2024-11-20T03:01:45.465839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce GTX 1050 Ti. Max memory = 3.94 GB.\n",
      "2.592 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45b8339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:01:45.619054Z",
     "iopub.status.busy": "2024-11-20T03:01:45.618075Z",
     "iopub.status.idle": "2024-11-20T03:03:56.916334Z",
     "shell.execute_reply": "2024-11-20T03:03:56.914452Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "3cf26aac-6042-4458-c4a6-d8849efb6a95",
    "papermill": {
     "duration": 131.347241,
     "end_time": "2024-11-20T03:03:56.919680",
     "exception": false,
     "start_time": "2024-11-20T03:01:45.572439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 240 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 4 | Total steps = 1\n",
      " \"-____-\"     Number of trainable parameters = 78,151,680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.264400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer_stats:\n",
      "TrainOutput(global_step=1, training_loss=2.264357328414917, metrics={'train_runtime': 127.4108, 'train_samples_per_second': 0.031, 'train_steps_per_second': 0.008, 'total_flos': 38026407051264.0, 'train_loss': 2.264357328414917, 'epoch': 0.016666666666666666})\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(\n",
    "    experiment_id=experiment.experiment_id,\n",
    "    nested=True,\n",
    "):\n",
    "    trainer_stats = sft_trainer.train(\n",
    "        resume_from_checkpoint=False,\n",
    "        trial=None,\n",
    "    )\n",
    "\n",
    "    print('trainer_stats:')\n",
    "    pprint(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15f8a81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:03:57.002289Z",
     "iopub.status.busy": "2024-11-20T03:03:57.001775Z",
     "iopub.status.idle": "2024-11-20T03:03:57.011672Z",
     "shell.execute_reply": "2024-11-20T03:03:57.010919Z"
    },
    "papermill": {
     "duration": 0.054582,
     "end_time": "2024-11-20T03:03:57.013070",
     "exception": false,
     "start_time": "2024-11-20T03:03:56.958488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.4108 seconds used for training.\n",
      "2.12 minutes used for training.\n",
      "Peak reserved memory = 2.693 GB.\n",
      "Peak reserved memory for training = 0.101 GB.\n",
      "Peak reserved memory % of max memory = 68.35 %.\n",
      "Peak reserved memory for training % of max memory = 2.563 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1c30e20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:03:57.080380Z",
     "iopub.status.busy": "2024-11-20T03:03:57.080036Z",
     "iopub.status.idle": "2024-11-20T03:03:57.085247Z",
     "shell.execute_reply": "2024-11-20T03:03:57.084634Z"
    },
    "papermill": {
     "duration": 0.039283,
     "end_time": "2024-11-20T03:03:57.086511",
     "exception": false,
     "start_time": "2024-11-20T03:03:57.047228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: right\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bb346ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:03:57.162509Z",
     "iopub.status.busy": "2024-11-20T03:03:57.162271Z",
     "iopub.status.idle": "2024-11-20T03:04:03.188048Z",
     "shell.execute_reply": "2024-11-20T03:04:03.187270Z"
    },
    "papermill": {
     "duration": 6.070953,
     "end_time": "2024-11-20T03:04:03.189738",
     "exception": false,
     "start_time": "2024-11-20T03:03:57.118785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt, response = generate_test_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "436a1262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:03.266534Z",
     "iopub.status.busy": "2024-11-20T03:04:03.266293Z",
     "iopub.status.idle": "2024-11-20T03:04:03.271279Z",
     "shell.execute_reply": "2024-11-20T03:04:03.270357Z"
    },
    "papermill": {
     "duration": 0.050156,
     "end_time": "2024-11-20T03:04:03.272987",
     "exception": false,
     "start_time": "2024-11-20T03:04:03.222831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<s> <|system|>\n",
      "You are an AI agent acting as a human assistant.\n",
      "\n",
      "You are aware of the following tools in your environment:\n",
      "{\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"function\": {\n",
      "        \"description\": \"Get the current weather\",\n",
      "        \"name\": \"get_current_weather\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country, eg. San Francisco, USA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\", \"format\"]}\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "If you would like to suggest one or more tool calls, please respond in the following format:\n",
      "{\n",
      "  \"finish_reason\": \"tool_calls\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"arguments\": \"{\\\"parameter_name\\\": \\\"parameter_value\\\"}\",\n",
      "      \"id\": \"call_id\",\n",
      "      \"name\": \"tool_name\"\n",
      "    }\n",
      "  ]\n",
      "}</s> \n",
      "<|user|>\n",
      "What's the weather like in San Francisco and New York?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ef9ec8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:03.354012Z",
     "iopub.status.busy": "2024-11-20T03:04:03.353709Z",
     "iopub.status.idle": "2024-11-20T03:04:03.358071Z",
     "shell.execute_reply": "2024-11-20T03:04:03.357236Z"
    },
    "papermill": {
     "duration": 0.051224,
     "end_time": "2024-11-20T03:04:03.359570",
     "exception": false,
     "start_time": "2024-11-20T03:04:03.308346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response (predicted, after training):\n",
      "I don't have access to real-time weather data, but according to the information available data, the weather in san francisco and new york are both hot and humid. San francisco has a maximum temperature of 328.8Â°c (992.8f) and minimum of 25.8Â°c (439.2f, while new york has maximum of 39.8Â° (434.2f and minimum 2.8Â° (32f).</s>\n"
     ]
    }
   ],
   "source": [
    "print('response (predicted, after training):')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "675562d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:03.435626Z",
     "iopub.status.busy": "2024-11-20T03:04:03.435302Z",
     "iopub.status.idle": "2024-11-20T03:04:03.440548Z",
     "shell.execute_reply": "2024-11-20T03:04:03.439549Z"
    },
    "papermill": {
     "duration": 0.048794,
     "end_time": "2024-11-20T03:04:03.442516",
     "exception": false,
     "start_time": "2024-11-20T03:04:03.393722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a92a7d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:03.512312Z",
     "iopub.status.busy": "2024-11-20T03:04:03.512075Z",
     "iopub.status.idle": "2024-11-20T03:04:04.914842Z",
     "shell.execute_reply": "2024-11-20T03:04:04.913866Z"
    },
    "papermill": {
     "duration": 1.439411,
     "end_time": "2024-11-20T03:04:04.917694",
     "exception": false,
     "start_time": "2024-11-20T03:04:03.478283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    # model.save_pretrained(\"lora_model\")  # Local saving\n",
    "    # model.save_pretrained(\"data/06_models/model/lora\")  # Local saving\n",
    "    model.save_pretrained(os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"))  # Local saving\n",
    "    # tokenizer.save_pretrained(\"lora_model\")\n",
    "    # tokenizer.save_pretrained(\"data/06_models/model/lora\")\n",
    "    tokenizer.save_pretrained(os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"))\n",
    "\n",
    "else:\n",
    "    # TODO: push to hub\n",
    "    raise NotImplementedError(\"Pushing to hub is not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4edb01e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:05.094823Z",
     "iopub.status.busy": "2024-11-20T03:04:05.094123Z",
     "iopub.status.idle": "2024-11-20T03:04:05.101460Z",
     "shell.execute_reply": "2024-11-20T03:04:05.099961Z"
    },
    "papermill": {
     "duration": 0.126139,
     "end_time": "2024-11-20T03:04:05.103968",
     "exception": false,
     "start_time": "2024-11-20T03:04:04.977829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f56b493f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:05.251677Z",
     "iopub.status.busy": "2024-11-20T03:04:05.251226Z",
     "iopub.status.idle": "2024-11-20T03:04:33.673399Z",
     "shell.execute_reply": "2024-11-20T03:04:33.671379Z"
    },
    "papermill": {
     "duration": 28.497513,
     "end_time": "2024-11-20T03:04:33.681735",
     "exception": false,
     "start_time": "2024-11-20T03:04:05.184222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 0.0 out of 15.5 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                   | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                           | 2/22 [00:00<00:01, 19.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                    | 5/22 [00:00<00:00, 23.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                            | 8/22 [00:00<00:00, 24.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                     | 11/22 [00:00<00:00, 24.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 14/22 [00:00<00:00, 24.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 17/22 [00:00<00:00, 24.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 20/22 [00:00<00:00, 25.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 24.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model.save_pretrained_merged(\n",
    "        os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/merged_16bit\"),\n",
    "        save_method=\"merged_16bit\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    # model.save_pretrained_merged(\n",
    "    #     os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/merged_4bit\"),\n",
    "    #     save_method=\"merged_4bit\",\n",
    "    #     tokenizer=tokenizer,\n",
    "    # )\n",
    "    model.save_pretrained_merged(\n",
    "        os.path.join(project_root, \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/merged_lora\"),\n",
    "        save_method=\"lora\",\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Pushing to hub is not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a207b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:33.990251Z",
     "iopub.status.busy": "2024-11-20T03:04:33.989681Z",
     "iopub.status.idle": "2024-11-20T03:04:33.998728Z",
     "shell.execute_reply": "2024-11-20T03:04:33.997579Z"
    },
    "papermill": {
     "duration": 0.096338,
     "end_time": "2024-11-20T03:04:34.000857",
     "exception": false,
     "start_time": "2024-11-20T03:04:33.904519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token: <unk>\n",
      "tokenizer.padding_side: left\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "314f2a01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:34.170966Z",
     "iopub.status.busy": "2024-11-20T03:04:34.170440Z",
     "iopub.status.idle": "2024-11-20T03:04:34.177738Z",
     "shell.execute_reply": "2024-11-20T03:04:34.175963Z"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "5b71f982-38c0-44c8-a4e5-58cd20b5a585",
    "papermill": {
     "duration": 0.10825,
     "end_time": "2024-11-20T03:04:34.179867",
     "exception": false,
     "start_time": "2024-11-20T03:04:34.071617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def chat_completion_request(\n",
    "#     documents: list,\n",
    "#     messages: list,\n",
    "#     tools: list,\n",
    "# ):\n",
    "#     FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "#     inputs = tokenizer.apply_chat_template(\n",
    "#         add_generation_prompt=True,\n",
    "#         conversation=messages,\n",
    "#         documents=documents,\n",
    "#         return_tensors=\"pt\",\n",
    "#         tokenize=True,\n",
    "#         tools=tools,\n",
    "#     ).to(device)\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         do_sample=False,\n",
    "#         input_ids=inputs,\n",
    "#         max_new_tokens=256,\n",
    "#         use_cache=True,\n",
    "#         # temperature=0.0,\n",
    "#     )\n",
    "\n",
    "#     batch_decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "#     choices: List[ChatChoice] = []\n",
    "\n",
    "#     for i in range(len(batch_decoded_outputs)):\n",
    "#         response = batch_decoded_outputs[i][len(tokenizer.decode(inputs[i])):].replace(tokenizer.eos_token, \"\") # TODO: skip special tokens when decoding instead?\n",
    "\n",
    "#         try:\n",
    "#             response = json.loads(response)\n",
    "\n",
    "#             finish_reason: str = response.get(\"finish_reason\")\n",
    "#             tool_calls_json = response.get(\"tool_calls\")\n",
    "#             tool_calls: List[ToolCall] = []\n",
    "\n",
    "#             for tool_call_json in tool_calls_json:\n",
    "#                 tool_call = ToolCall(\n",
    "#                     function=FunctionToolCallArguments(\n",
    "#                         arguments=tool_call_json.get(\"arguments\"),\n",
    "#                         name=tool_call_json.get(\"name\"),\n",
    "#                     ),\n",
    "#                     id=tool_call_json.get(\"id\"),\n",
    "#                     type=\"function\",\n",
    "#                 )\n",
    "\n",
    "#                 tool_calls.append(tool_call)\n",
    "\n",
    "#             message: ChatMessage = ChatMessage(\n",
    "#                 role=\"assistant\",\n",
    "#                 tool_calls=tool_calls,\n",
    "#             )\n",
    "\n",
    "#         except json.JSONDecodeError:\n",
    "#             finish_reason: str = \"stop\"\n",
    "#             message: ChatMessage = ChatMessage(\n",
    "#                 role=\"assistant\",\n",
    "#                 content=response,\n",
    "#             )\n",
    "\n",
    "#         choices.append(\n",
    "#             ChatChoice(\n",
    "#                 index=i,\n",
    "#                 finish_reason=finish_reason,\n",
    "#                 logprobs=None,\n",
    "#                 message=message,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     return ChatResponse(\n",
    "#         choices=choices,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "720c4219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:34.374318Z",
     "iopub.status.busy": "2024-11-20T03:04:34.373856Z",
     "iopub.status.idle": "2024-11-20T03:04:34.403224Z",
     "shell.execute_reply": "2024-11-20T03:04:34.400372Z"
    },
    "papermill": {
     "duration": 0.140039,
     "end_time": "2024-11-20T03:04:34.407667",
     "exception": false,
     "start_time": "2024-11-20T03:04:34.267628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import mlflow\n",
    "from mlflow.models import set_model\n",
    "from mlflow.pyfunc import ChatModel\n",
    "from mlflow.types.llm import (\n",
    "    ChatChoice,\n",
    "    ChatMessage,\n",
    "    ChatParams,\n",
    "    ChatResponse,\n",
    "    FunctionToolCallArguments,\n",
    "    ToolCall,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "import os\n",
    "\n",
    "project_root = os.getcwd()\n",
    "\n",
    "while not os.path.exists(os.path.join(project_root, \"register_prefect_flow.py\")):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "max_seq_length = 4096\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"data/06_models/model/lora\"\n",
    "# model_name = \"data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0_lora_sft\"\n",
    "model_name = f\"{project_root}/data/06_models/mjschock/TinyLlama-1.1B-Chat-v1.0-tool-calling-sft/lora\"\n",
    "\n",
    "class Client():\n",
    "    def __init__(self):\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            max_seq_length=max_seq_length,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "\n",
    "        FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def chat_completion_request(\n",
    "        self,\n",
    "        documents: list,\n",
    "        messages: list,\n",
    "        tools: list,\n",
    "    ):\n",
    "        # FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "        inputs = self.tokenizer.apply_chat_template(\n",
    "            add_generation_prompt=True,\n",
    "            conversation=messages,\n",
    "            documents=documents,\n",
    "            return_tensors=\"pt\",\n",
    "            tokenize=True,\n",
    "            tools=tools,\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            do_sample=False,\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=256,\n",
    "            use_cache=True,\n",
    "            # temperature=0.0,\n",
    "        )\n",
    "\n",
    "        batch_decoded_outputs = self.tokenizer.batch_decode(outputs)\n",
    "\n",
    "        choices: List[ChatChoice] = []\n",
    "\n",
    "        for i in range(len(batch_decoded_outputs)):\n",
    "            response = batch_decoded_outputs[i][len(self.tokenizer.decode(inputs[i])):].replace(self.tokenizer.eos_token, \"\") # TODO: skip special tokens when decoding instead?\n",
    "\n",
    "            try:\n",
    "                response = json.loads(response)\n",
    "\n",
    "                finish_reason: str = response.get(\"finish_reason\")\n",
    "                tool_calls_json = response.get(\"tool_calls\")\n",
    "                tool_calls: List[ToolCall] = []\n",
    "\n",
    "                for tool_call_json in tool_calls_json:\n",
    "                    tool_call = ToolCall(\n",
    "                        function=FunctionToolCallArguments(\n",
    "                            arguments=tool_call_json.get(\"arguments\"),\n",
    "                            name=tool_call_json.get(\"name\"),\n",
    "                        ),\n",
    "                        id=tool_call_json.get(\"id\"),\n",
    "                        type=\"function\",\n",
    "                    )\n",
    "\n",
    "                    tool_calls.append(tool_call)\n",
    "\n",
    "                message: ChatMessage = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    tool_calls=tool_calls,\n",
    "                )\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                finish_reason: str = \"stop\"\n",
    "                message: ChatMessage = ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=response,\n",
    "                )\n",
    "\n",
    "            choices.append(\n",
    "                ChatChoice(\n",
    "                    index=i,\n",
    "                    finish_reason=finish_reason,\n",
    "                    logprobs=None,\n",
    "                    message=message,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return ChatResponse(\n",
    "            choices=choices,\n",
    "        )\n",
    "\n",
    "class Model(ChatModel):\n",
    "    def __init__(self):\n",
    "        # self.model_name = \"llama3.2:1b\"\n",
    "        self.client = None\n",
    "\n",
    "    def load_context(self, context):\n",
    "        # self.model_name = \"llama3.2:1b\"\n",
    "        # self.client = ollama.Client()\n",
    "        self.client = Client()\n",
    "\n",
    "    # the core method that needs to be implemented. this function\n",
    "    # will be called every time a user sends messages to our model\n",
    "    # @mlflow.trace(span_type=SpanType.AGENT)\n",
    "    def predict(self, context, messages: list[ChatMessage], params: ChatParams):\n",
    "        # instantiate the OpenAI client\n",
    "        # client = OpenAI()\n",
    "\n",
    "        # convert the messages to a format that the OpenAI API expects\n",
    "        messages = [m.to_dict() for m in messages]\n",
    "\n",
    "        print('params:')\n",
    "        pprint(params.to_dict())\n",
    "\n",
    "        tools = params.tools or []\n",
    "\n",
    "        print('tools:')\n",
    "        pprint(tools)\n",
    "\n",
    "        tools = [t.to_dict() for t in tools]\n",
    "\n",
    "        print('tools:')\n",
    "        pprint(tools)\n",
    "\n",
    "        # call the OpenAI API\n",
    "        # response = client.chat.completions.create(\n",
    "        #     model=\"gpt-4o-mini\",\n",
    "        #     messages=messages,\n",
    "        #     # pass the tools in the request\n",
    "        #     tools=self.tools,\n",
    "        # )\n",
    "\n",
    "        response = self.client.chat_completion_request(\n",
    "            documents=[], # we don't need documents for this example\n",
    "            messages=messages,\n",
    "            # tools=self.tools,\n",
    "            # tools=[],\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        # return the result as a ChatResponse, as this\n",
    "        # is the expected output of the predict method\n",
    "        return ChatResponse.from_dict(response.to_dict())\n",
    "\n",
    "set_model(Model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f296fea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:34.580079Z",
     "iopub.status.busy": "2024-11-20T03:04:34.579447Z",
     "iopub.status.idle": "2024-11-20T03:04:34.932678Z",
     "shell.execute_reply": "2024-11-20T03:04:34.931319Z"
    },
    "papermill": {
     "duration": 0.442065,
     "end_time": "2024-11-20T03:04:34.935246",
     "exception": false,
     "start_time": "2024-11-20T03:04:34.493181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34229"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del sft_trainer\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del tokenizer\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "365b6763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:04:35.139080Z",
     "iopub.status.busy": "2024-11-20T03:04:35.138466Z",
     "iopub.status.idle": "2024-11-20T03:05:32.689845Z",
     "shell.execute_reply": "2024-11-20T03:05:32.687021Z"
    },
    "papermill": {
     "duration": 57.671775,
     "end_time": "2024-11-20T03:05:32.693412",
     "exception": false,
     "start_time": "2024-11-20T03:04:35.021637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 19:04:35 INFO mlflow.pyfunc: Predicting on input example to validate output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/mjschock/Projects/training-tinyllama-for-tool-calling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/tinyllama-chat-bnb-4bit can only handle sequence lengths of at most 2048.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 4096!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "{'n': 1,\n",
      " 'stream': False,\n",
      " 'temperature': 1.0,\n",
      " 'tools': [{'function': {'description': 'Get the current weather',\n",
      "                         'name': 'get_current_weather',\n",
      "                         'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                           'fahrenheit'],\n",
      "                                                                  'type': 'string'},\n",
      "                                                       'location': {'description': 'The '\n",
      "                                                                                   'city '\n",
      "                                                                                   'and '\n",
      "                                                                                   'country, '\n",
      "                                                                                   'eg. '\n",
      "                                                                                   'San '\n",
      "                                                                                   'Francisco, '\n",
      "                                                                                   'USA',\n",
      "                                                                    'type': 'string'}},\n",
      "                                        'required': ['location', 'format'],\n",
      "                                        'type': 'object'},\n",
      "                         'strict': False},\n",
      "            'type': 'function'}]}\n",
      "tools:\n",
      "[ToolDefinition(function=FunctionToolDefinition(name='get_current_weather',\n",
      "                                                description='Get the current '\n",
      "                                                            'weather',\n",
      "                                                parameters=ToolParamsSchema(properties={'format': ParamProperty(type='string',\n",
      "                                                                                                                description=None,\n",
      "                                                                                                                enum=['celsius',\n",
      "                                                                                                                      'fahrenheit'],\n",
      "                                                                                                                items=None),\n",
      "                                                                                        'location': ParamProperty(type='string',\n",
      "                                                                                                                  description='The '\n",
      "                                                                                                                              'city '\n",
      "                                                                                                                              'and '\n",
      "                                                                                                                              'country, '\n",
      "                                                                                                                              'eg. '\n",
      "                                                                                                                              'San '\n",
      "                                                                                                                              'Francisco, '\n",
      "                                                                                                                              'USA',\n",
      "                                                                                                                  enum=None,\n",
      "                                                                                                                  items=None)},\n",
      "                                                                            type='object',\n",
      "                                                                            required=['location',\n",
      "                                                                                      'format'],\n",
      "                                                                            additionalProperties=None),\n",
      "                                                strict=False),\n",
      "                type='function')]\n",
      "tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'},\n",
      "               'strict': False},\n",
      "  'type': 'function'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590efb165b674979b8c99dde39522445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/mjschock/Projects/training-tinyllama-for-tool-calling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce GTX 1050 Ti. Max memory: 3.94 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 6.1. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "{'n': 1,\n",
      " 'stream': False,\n",
      " 'temperature': 1.0,\n",
      " 'tools': [{'function': {'description': 'Get the current weather',\n",
      "                         'name': 'get_current_weather',\n",
      "                         'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                           'fahrenheit'],\n",
      "                                                                  'type': 'string'},\n",
      "                                                       'location': {'description': 'The '\n",
      "                                                                                   'city '\n",
      "                                                                                   'and '\n",
      "                                                                                   'country, '\n",
      "                                                                                   'eg. '\n",
      "                                                                                   'San '\n",
      "                                                                                   'Francisco, '\n",
      "                                                                                   'USA',\n",
      "                                                                    'type': 'string'}},\n",
      "                                        'required': ['location', 'format'],\n",
      "                                        'type': 'object'},\n",
      "                         'strict': False},\n",
      "            'type': 'function'}]}\n",
      "tools:\n",
      "[ToolDefinition(function=FunctionToolDefinition(name='get_current_weather',\n",
      "                                                description='Get the current '\n",
      "                                                            'weather',\n",
      "                                                parameters=ToolParamsSchema(properties={'format': ParamProperty(type='string',\n",
      "                                                                                                                description=None,\n",
      "                                                                                                                enum=['celsius',\n",
      "                                                                                                                      'fahrenheit'],\n",
      "                                                                                                                items=None),\n",
      "                                                                                        'location': ParamProperty(type='string',\n",
      "                                                                                                                  description='The '\n",
      "                                                                                                                              'city '\n",
      "                                                                                                                              'and '\n",
      "                                                                                                                              'country, '\n",
      "                                                                                                                              'eg. '\n",
      "                                                                                                                              'San '\n",
      "                                                                                                                              'Francisco, '\n",
      "                                                                                                                              'USA',\n",
      "                                                                                                                  enum=None,\n",
      "                                                                                                                  items=None)},\n",
      "                                                                            type='object',\n",
      "                                                                            required=['location',\n",
      "                                                                                      'format'],\n",
      "                                                                            additionalProperties=None),\n",
      "                                                strict=False),\n",
      "                type='function')]\n",
      "tools:\n",
      "[{'function': {'description': 'Get the current weather',\n",
      "               'name': 'get_current_weather',\n",
      "               'parameters': {'properties': {'format': {'enum': ['celsius',\n",
      "                                                                 'fahrenheit'],\n",
      "                                                        'type': 'string'},\n",
      "                                             'location': {'description': 'The '\n",
      "                                                                         'city '\n",
      "                                                                         'and '\n",
      "                                                                         'country, '\n",
      "                                                                         'eg. '\n",
      "                                                                         'San '\n",
      "                                                                         'Francisco, '\n",
      "                                                                         'USA',\n",
      "                                                          'type': 'string'}},\n",
      "                              'required': ['location', 'format'],\n",
      "                              'type': 'object'},\n",
      "               'strict': False},\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_experiment(\"chatmodel-quickstart\")\n",
    "# code_path = \"ollama_model.py\"\n",
    "code_path = \"model.py\"\n",
    "\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment.experiment_id,\n",
    "    nested=True,\n",
    "):\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        # \"ollama_model\",\n",
    "        \"model\",\n",
    "        python_model=code_path,\n",
    "        input_example={\n",
    "            # \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "            \"messages\": test_example_messages[0:1],\n",
    "            \"tools\": test_example_tools,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "057fcaee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:32.849381Z",
     "iopub.status.busy": "2024-11-20T03:05:32.848924Z",
     "iopub.status.idle": "2024-11-20T03:05:32.857241Z",
     "shell.execute_reply": "2024-11-20T03:05:32.855135Z"
    },
    "papermill": {
     "duration": 0.073508,
     "end_time": "2024-11-20T03:05:32.860375",
     "exception": false,
     "start_time": "2024-11-20T03:05:32.786867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs:/2e32883ccfc24e479e99ec336bd99628/model'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93cd653e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:32.968643Z",
     "iopub.status.busy": "2024-11-20T03:05:32.968341Z",
     "iopub.status.idle": "2024-11-20T03:05:32.974692Z",
     "shell.execute_reply": "2024-11-20T03:05:32.972424Z"
    },
    "papermill": {
     "duration": 0.063218,
     "end_time": "2024-11-20T03:05:32.977723",
     "exception": false,
     "start_time": "2024-11-20T03:05:32.914505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "# result = loaded_model.predict(\n",
    "#     data={\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}],\n",
    "#         \"max_tokens\": 25,\n",
    "#     }\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "590870e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.094741Z",
     "iopub.status.busy": "2024-11-20T03:05:33.094410Z",
     "iopub.status.idle": "2024-11-20T03:05:33.098727Z",
     "shell.execute_reply": "2024-11-20T03:05:33.097629Z"
    },
    "papermill": {
     "duration": 0.052884,
     "end_time": "2024-11-20T03:05:33.100301",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.047417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat_response = chat_completion_request(\n",
    "#     documents=test_example_documents,\n",
    "#     messages=test_example_messages[0:1],\n",
    "#     tools=test_example_tools,\n",
    "# )\n",
    "\n",
    "# chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1f08907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.174293Z",
     "iopub.status.busy": "2024-11-20T03:05:33.174005Z",
     "iopub.status.idle": "2024-11-20T03:05:33.178177Z",
     "shell.execute_reply": "2024-11-20T03:05:33.177138Z"
    },
    "papermill": {
     "duration": 0.042495,
     "end_time": "2024-11-20T03:05:33.179549",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.137054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat_response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fff079b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.262026Z",
     "iopub.status.busy": "2024-11-20T03:05:33.261694Z",
     "iopub.status.idle": "2024-11-20T03:05:33.265579Z",
     "shell.execute_reply": "2024-11-20T03:05:33.264580Z"
    },
    "papermill": {
     "duration": 0.04936,
     "end_time": "2024-11-20T03:05:33.267099",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.217739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('tokenizer.pad_token:', tokenizer.pad_token)\n",
    "# print('tokenizer.padding_side:', tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ec8966e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.377135Z",
     "iopub.status.busy": "2024-11-20T03:05:33.376650Z",
     "iopub.status.idle": "2024-11-20T03:05:33.382610Z",
     "shell.execute_reply": "2024-11-20T03:05:33.380709Z"
    },
    "papermill": {
     "duration": 0.065191,
     "end_time": "2024-11-20T03:05:33.385550",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.320359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# model.save_pretrained_merged(\n",
    "#     \"data/06_models/model/finetuned\",\n",
    "#     save_method=\"merged_16bit\",\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98fdb0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.531468Z",
     "iopub.status.busy": "2024-11-20T03:05:33.531083Z",
     "iopub.status.idle": "2024-11-20T03:05:33.535882Z",
     "shell.execute_reply": "2024-11-20T03:05:33.534499Z"
    },
    "papermill": {
     "duration": 0.0732,
     "end_time": "2024-11-20T03:05:33.539804",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.466604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.paddding_side = \"right\"\n",
    "\n",
    "# # Get the ID of the MLflow Run that was automatically created above\n",
    "# # last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "# # Save a tokenizer without padding because it is only needed for training\n",
    "# # tokenizer_no_pad = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True)\n",
    "\n",
    "# with mlflow.start_run(\n",
    "#     nested=True,\n",
    "#     # run_id=last_run_id,\n",
    "# ):\n",
    "#     mlflow.transformers.log_model(\n",
    "#         # transformers_model={\"model\": sft_trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "#         transformers_model={\"model\": sft_trainer.model, \"tokenizer\": sft_trainer.tokenizer},\n",
    "#         # prompt_template=prompt_template,\n",
    "#         # signature=signature,\n",
    "#         artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53cf9115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.690333Z",
     "iopub.status.busy": "2024-11-20T03:05:33.689604Z",
     "iopub.status.idle": "2024-11-20T03:05:33.697669Z",
     "shell.execute_reply": "2024-11-20T03:05:33.695974Z"
    },
    "papermill": {
     "duration": 0.068688,
     "end_time": "2024-11-20T03:05:33.700172",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.631484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If running in a Jupyter or Databricks notebook cell, uncomment the following line:\n",
    "# %%writefile \"./basic.py\"\n",
    "\n",
    "# import pandas as pd\n",
    "# from typing import List, Dict\n",
    "# from mlflow.pyfunc import PythonModel\n",
    "# from mlflow.models import set_model\n",
    "\n",
    "\n",
    "# class BasicModel(PythonModel):\n",
    "#     def exponential(self, numbers):\n",
    "#         return {f\"{x}\": 2**x for x in numbers}\n",
    "\n",
    "#     def predict(self, context, model_input) -> Dict[str, float]:\n",
    "#         if isinstance(model_input, pd.DataFrame):\n",
    "#             model_input = model_input.to_dict()[0].values()\n",
    "#         return self.exponential(model_input)\n",
    "\n",
    "\n",
    "# # Specify which definition in this script represents the model instance\n",
    "# set_model(BasicModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0da5cebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T03:05:33.872542Z",
     "iopub.status.busy": "2024-11-20T03:05:33.871800Z",
     "iopub.status.idle": "2024-11-20T03:05:33.879931Z",
     "shell.execute_reply": "2024-11-20T03:05:33.878437Z"
    },
    "papermill": {
     "duration": 0.11355,
     "end_time": "2024-11-20T03:05:33.882143",
     "exception": false,
     "start_time": "2024-11-20T03:05:33.768593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# mlflow.set_experiment(\"Basic Model From Code\")\n",
    "\n",
    "# model_path = \"basic.py\"\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     model_info = mlflow.pyfunc.log_model(\n",
    "#         python_model=model_path,  # Define the model as the path to the script that was just saved\n",
    "#         artifact_path=\"arithemtic_model\",\n",
    "#         input_example=[42.0, 24.0],\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 313.957881,
   "end_time": "2024-11-20T03:05:36.809029",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/training-tinyllama-for-tool-calling.ipynb",
   "output_path": "notebooks/training-tinyllama-for-tool-calling.output.ipynb",
   "parameters": {},
   "start_time": "2024-11-20T03:00:22.851148",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09c4ef78a7bb4b70a8b3fb99ad58dbb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "525621c297604fe29675e693612ff5fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "590efb165b674979b8c99dde39522445": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82398b2df1184e54b212be0fb2bf1a43",
        "IPY_MODEL_92d04c3ef6eb482a8b6eae985d129272",
        "IPY_MODEL_8e68246313e74727b4fc10a4d6387a76"
       ],
       "layout": "IPY_MODEL_6f9fecc6ad76459982f24a8718840f0d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "65f8f7e295314ccbb0abecfc08b297b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6f9fecc6ad76459982f24a8718840f0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81e0b79f7ff842b78cfc43bff45a9bf0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82398b2df1184e54b212be0fb2bf1a43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f91f2e44bc084908b2de6593f7c6a503",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_09c4ef78a7bb4b70a8b3fb99ad58dbb0",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloadingâ€‡artifacts:â€‡100%"
      }
     },
     "8e68246313e74727b4fc10a4d6387a76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a17af7bb71bb4a31a1dc1bfcae1c46b6",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_65f8f7e295314ccbb0abecfc08b297b6",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡7/7â€‡[00:00&lt;00:00,â€‡397.10it/s]"
      }
     },
     "92d04c3ef6eb482a8b6eae985d129272": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81e0b79f7ff842b78cfc43bff45a9bf0",
       "max": 7.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_525621c297604fe29675e693612ff5fd",
       "tabbable": null,
       "tooltip": null,
       "value": 7.0
      }
     },
     "a17af7bb71bb4a31a1dc1bfcae1c46b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f91f2e44bc084908b2de6593f7c6a503": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}